\section{A Quick Refresher on Analysis and Probability}

% Exercise 1.1
\section*{Exercise 1.1}
Let $x_1, x_2 \in \text{conv}(T)$, and $\lambda \in [0, 1]$. Then there exists $j, k \in \mathbb{N}$ such that 
\[ x_1 = a_1 y_1 + \cdots + a_j y_j, a_i \geq 0, \sum_{i = 1}^{j} a_i = 1, \]
\[ x_2 = b_1 z_1 + \cdots + b_k z_k, b_I \geq 0, \sum_{i = 1}^{k} b_i = 1. \]
Then we get 
\[ \lambda x_1 + (1 - \lambda) x_2 
= \lambda \sum_{i = 1}^{j} a_i y_i + (1 - \lambda)\sum_{i = 1}^{k} b_i z_i. \]
From the formulation above, 
\[ a_i \geq 0 \implies \lambda a_i \geq 0, \ b_i \geq 0 \implies (1 - \lambda) b_i \geq 0. \]
Moreover, when summing up the coefficients, 
\[ \lambda \sum_{i = 1}^{j} a_i + (1 - \lambda) \sum_{i = 1}^{k} b_i = \lambda + (1 - \lambda) = 1. \]
Therefore $\lambdax_1 + (1 - \lambda)x_2 \in \text{conv}(T)$. Here we assumed that without loss of generality, 
there are no shared 
points between $x_1$ and $x_2$. If there were to be shared points, it would not have affected our analysis 
because each coefficient 
in the convex combination will still be greater than 0, and also their sum will be 1.


% Exercise 1.2
\newpage
\section*{Exercise 1.2}
Let $f_1, \dots, f_m$ be convex functions, and $g: K \to \mathbb{R}$ be defined as 
\[ g(x) = \max_{x}(f_1(x), \cdots, f_m(x)). \]
Let $x, y \in K$, and let $\lambda \in [0, 1]$. Then 
\begin{align*}
	g(\lambda x + (1 - \lambda)y) 
	&= \max_{}(f_1(\lambda x + (1 - \lambda)y), \cdots, f_m(\lambda x + (1 - \lambda)y)) \\
	&\leq \max_{}(\lambda f_1(x) + (1 - \lambda)f_1(y), \cdots, \lambda f_m(x) + (1 - \lambda) f_m(y)) \\
	&\leq \max_{}(\lambda f_1(x), \cdots, \lambda f_m(x)) + \max_{}((1 - \lambda)f_1(y), \cdots, (1 - \lambda)f_m(y)) \\
	&= \lambda \max_{}(f_1(x), \cdots, f_m(x)) + (1 - \lambda) \max_{}(f_1(y), \cdots, f_m(y)) \\
	&= \lambda g(x) + 1 - \lambda) g(y).
\end{align*}
Therefore $g$ is a convex function.


% Exercise 1.3
\newpage
\section*{Exercise 1.3}
\subsection*{(a)}
($\implies$) Suppose that $f$ is convex. For the base case, when $m = 2$, by the definition of convexity, 
the statement is true. For the inductive hypothesis, assume that for some $m \in \mathbb{N}$, 
\[ f \biggl( \sum_{i = 1}^{m} \lambda_i x_i \biggr) \leq \sum_{i = 1}^{m} \lambda_i f(x_i), \lambda_1 \geq 0, 
\sum_{i = 1}^{m} \lambda_i = 1. \]
With $\lambda_j \geq 0, \sum_{j = 0}^{m + 1} \lambda_j = 1$, without loss of generality assume that 
$\lambda_{m + 1} < 1$ (if not we can switch to another $\lambda$ that satisfies this condition). 
\begin{align*}
	f \biggl( \sum_{i = 1}^{m + 1} \lambda_i x_i \biggr) 
	&= f \biggl( (1 - \lambda_{m + 1}) \sum_{j = 1}^{m} \frac{\lambda_j}{1 - \lambda_{j + 1}} x_j + \lambda_{m + 1} x_{m + 1} \biggr) \\
	&\leq (1 - \lambda_{m + 1})  f \biggl( (1 - \lambda_{m + 1}) \sum_{j = 1}^{m} \frac{\lambda_j}{1 - \lambda_{j + 1}} x_j \biggr) 
	+ \lambda_{m + 1} f(x_{m + 1}) \quad \text{(Base case)} \\
	&\leq (1 - \lambda_{m + 1}) \sum_{j = 1}^{m} \frac{\lambda_j}{1 - \lambda_{m + 1}}f(x_j) + \lambda_{m + 1}f(x_{m + 1}) \quad 
	\text{(Inductive step)} \\
	&= \sum_{j = 1}^{m} \lambda_j f(x_j) + \lambda_{m + 1} f(x_{m + 1}) \\
	&= \sum_{j = 1}^{m + 1} \lambda_j f(x_j).
\end{align*}
($\impliedby$) Take $m = 2$ and we are done.

\subsection*{(b)}
By the definition given for $X$,, let 
\[ P(X = x_i) = p_i, i = 1, \dots, n, \ p_i \geq 0, \sum_{i = 1}^{n} p_i = 1. \]
We can directly see from our construction that 
\[ \mathbb{E}[X] = \sum_{i = 1}^{n} p_i x_i. \]
Then from part (a), 
\[ f(\mathbb{E}[X]) = f \biggl( \sum_{i = 1}^{n} p_i x_i \biggr) 
\leq \sum_{i = 1}^{n} p_i f(x_i) = \mathbb{E}[f(X)]. \]


% Exercise 1.4
\newpage
\section*{Exercise 1.4}
Let $x \in \text{conv}(T)$. Then for some $m \in \mathbb{N}$, 
\[ x = \lambda_1 z_1 + \cdots + \lambda_m z_m, \ \lambda_i \geq 0, \sum_{i = 1}^{m} \lambda_i = 1. \]
Then by Jensen's Inequality from Exercise 3, 
\[ f(x) = f \biggl( \sum_{i = 1}^{m} \lambda_i z_i \biggr) \leq \sum_{i = 1}^{m} \lambda_i f(z_i) \leq \sup_{i} f(z_i). \]
Therefore we get 
\[ \sup_{x \in \text{conv}(T)} f(x) \leq \sup_{x \in T} f(x). \]
The other side ("$\geq$") is obvious because $T \subseteq \text{conv}(T)$. Therefore we get the equality.


% Exercise 1.5
\newpage
\section*{Exercise 1.5}
We'll proceed via proof by induction. For the base case when $n = 1$, let $x \in [-1. 1]$. Then $x$ can be written 
as a combination via 
\[ x = \frac{1 + x}{2} \cdot 1 + \frac{1 - x}{2} \cdot (-1). \]
For the inductive step, assume if $x \in [-1, 1]^n$, $x \in \text{conv}(\{ -1, 1 \}^n)$. Now let's consider 
$x \in [-1, 1]^{n + 1} = (x_1, \cdots, x_{n + 1})$. For a fixed value of $x_{n + 1} \in [-1, 1]$, from the 
induction hypothesis, $x \in \text{conv}(\{ -1, 1 \}^n)$. Then 
\begin{align*}
	x 
	&x_1, \cdots, x_{n + 1}) \\
	&= \frac{1 + x_{n + 1}}{2}(x_1, \cdots, x_n, 1) + \frac{1 - x_{n + 1}}{2}(x_1, \cdots, x_n, -1).
\end{align*}
Therefore $x$ is a convex combination of points from a convex combination (we can acheive that via normalizing), hence 
$x \in \text{conv}(\{ -1, 1 \}^{n + 1})$ so $[-1,1]^n \subseteq \text{conv}(\{-1, 1\}^n)$.

For the other side of the proof, let $x \in \text{conv}(\{ -1, 1 \}^n)$. Then $\exists m \leq 2^n$ such that 
\[ x = \lambda_1 z_1 + \cdot + \lambda_m z_m, z_i \in \text{conv}(\{ -1, 1 \}^n), \lambda_i \geq 0, 
\sum_{i = 1}^{m} \lambda_1 = 1. \]
Each entry $x_1$ satisfies $-1 \leq x_i \leq 1$ and equality occurs when all corresponding entries in $z_i$ are 
either $1$ or $-1$, hence $\text{conv}(\{-1, 1\}^n) \subseteq [-1, 1]^n$.

Finally we conclude that $\text{conv}(\{-1, 1\}^n) = [-1, 1]^n$.


% Exercise 1.6
\newpage
\section*{Exercise 1.6}
Let $x \in B_1^n$ so $\sum_{i = 1}^{n} |x_i| \leq 1$. Define the following sets: 
\[ I_+ = \{ i \in \{1, \cdots, n\}: x_i > 0 \}, I_- = \{ i \in \{1, \cdots, n\}: x_i < 0. \]
Without loss of generality, assume either $|I_+| > 0$ or $I_- > 0$. If both are zero, $x$ has to be 
the origin, which finding a convex combination from the standard bases vectors would be very easy.
Define 
\[ \lambda_{i+} = \begin{cases}
	|x_i| &\text{ if } \ i \in I_+, \\
	0 &\text{ if } \ i \in I_-, \\
	\frac{1}{2(|I_-| + |I_+|)} \biggl( 1 - \sum_{i \in I_- \cup I_+}^{} |x_i| \biggr) &\text{otherwise}
\end{cases}, \]
\[ \lambda_{i+} = \begin{cases}
	|x_i| &\text{ if } \ i \in I_-, \\
	0 &\text{ if } \ i \in I_+, \\
	\frac{1}{2(|I_-| + |I_+|)} \biggl( 1 - \sum_{i \in I_- \cup I_+}^{} |x_i| \biggr) &\text{otherwise}
\end{cases}. \]
Then, 
\[ x = \sum_{i = 1}^{n} \lambda_{i+}e_i + \lambda_{i-} (-e_i), \ \lambda_{i+}, \lambda_{i-} \geq 0, \ \sum_{i = 1}^{n} 
\lambda_{i+} + \lambda_{i-} = 1. \]
Hence $x \in \text{conv}(\{\pm e_1, \cdots, \pm e_n\})$ so $B_1^n \subseteq \text{conv}(\{\pm e_1, \cdots, \pm e_n\})$.

Now let $x \in \text{conv}(\{\pm e_1, \cdots, \pm e_n\})$. Then $\exists \lambda_{i+}, \lambda_{i-} \geq 0$ and summing 
to 1 such that 
\begin{align*}
	x 
	&= \lambda_{1+} e_1 + \cdots + \lambda_{n+} e_n + \lambda_{1-} (-e_1) + \cdots + \lambda_{n-} (-e_n) \\
	&\leq |\lambda_{1+} e_1| + \cdot + |\lambda_{n+} e_n| + |\lambda_{1-} e_1| + |\lambda_{n-} e_n| \\
	&= \sum_{i = 1}^{n} |\lambda_{i+}| + |\lambda_{i-}| \\
	&= 1.
\end{align*}
Therefore $x \in B_1^n$ so $\text{conv}(\{\pm e_1, \cdots, \pm e_n\}) \in B_1^n$. We conclude that 
$B_1^n = \text{conv}(\{\pm e_1, \cdots, \pm e_n\})$. 


% Exercise 1.7
\newpage
\section*{Exercise 1.7}
Denote $E_i = $event that freshman $i$ has no friends, $X = $number of freshman. Then we are bounding
\begin{align*}
	\sum_{n = 0}^{\infty} P \biggl( \bigcup_{i = 1}^X E_i \bigg| X = n \biggr) P(x = n)
	&= \sum_{n = 0}^{\infty} P \biggl( \bigcup_{i = 1}^n E_i \biggr) P(x = n) \\
	&\leq \sum_{n = 1}^{\infty} \frac{\lambda^n e^{-\lambda}}{n!} \sum_{i = 1}^{n} P(E_i) \\
	&= \sum_{n = 1}^{\infty} \frac{\lambda^n e^{-\lambda}}{n!} \cdot n(1 - p)^{n - 1} \\
	&= \sum_{n = 1}^{\infty} \frac{\lambda^n e^{-\lambda}}{(n - 1)!} (1 - p){n - 1} \\
	&= \lambda \sum_{n = 0}^{\infty} \frac{\lambda^n e^{-\lambda}}{n!}(1 - p)^n \\
	&= \lambda e^{-p \lambda}.
\end{align*}

From the question, since $p \geq 2 \ln{\lambda} / \lambda$, 
\[ \lambda e^{-p \lambda} \leq \lambda e^{-2 \ln{\lambda}} = \frac{1}{\lambda}. \]


% Exercise 1.8
\newpage
\section*{Exercise 1.8}
	

% Exercise 1.9
\newpage
\section*{Exercise 1.9}
Let $E_i = $ the event that student $i$ has no friends, and 
$B = \{ \text{there exists a friendless student} \}$. We are bounding the probability 
\[ P(B) 
= P \biggl( \bigcup_{i = 1}^n E_i \biggr) 
\leq \sum_{i = 1}^{n} P(E_i) 
= n(1 - p_n)^{n - 1}
\]
Now when we take the limit, 
\[ \lim_{n \to \infty} n(1 - p_n)^{n - 1} 
< \lim_{n \to \infty} n \biggl( \frac{(1 + \varepsilon) \ln{n}}{n} \biggr)^{n - 1} 
\to 0 \text{  as  } n \to \infty. \]


% Exercise 1.10
\newpage
\section*{Exercise 1.10}
\subsection*{(a)}
From definition, 
\[ \mathbb{E}\left[ X_i \right] = (1 - p_n)^{n - 1} \implies 
\mathbb{E}\left[ S_n \right] = n(1 - p_n)^{n - 1}. \]
We have that 
\begin{align*}
	\mathbb{E}\left[ S_n \right] 
	&= n(1 - p_n)^{n - 1} \\
	&< n \left( 1 - \frac{(1 - \varepsilon)\ln{n}}{n} \right)^{n - 1} \\
	&\to n \cdot e^{-(1 - \varepsilon)\ln{n}} \\
	&= \lim_{n \to \infty} n^{\varepsilon} \\
	&= +\infty.
\end{align*}

\subsection*{(b)}
First, let's look at the random variable $X_i X_j$. This is the indicator variable that both student $i$ and $j$ 
are friendless. For student $i$ and $j$, to be friendless, they cannot be friends with each of the the other 
$n - 1$ students, but we double counted the relationship between student $i$ and $j$! Therefore, 
\[ \mathbb{E}\left[ X_i X_j \right] = (1 - p_n)^{2n - 3} \quad (i \neq j). \]
The random variable $X_i^2$ is the same as $X_i$ as both represent the indicator that student $i$ is friendless. 
Therefore, we have 
\begin{align*}
	\mathbb{E}\left[ S_n^2 \right] 
	&= \sum_{i = 1}^{n} \mathbb{E}\left[ X_i^2 \right] + \sum_{i \neq j}^{} \mathbb{E}\left[ X_iX_j \right] \\
	&= n(1 - p_n)^{n - 1} + n(n - 1)(1 - p_n)^{2n - 3}.
\end{align*}
Then, we can calculate the variance of $S_n$:
\begin{align*}
	\mathrm{Var}(S_n) 
	&= \mathbb{E}\left[ S_n^2 \right] - (\mathbb{E}\left[ S_n \right])^2 \\
	&= n(1 - p_n)^{n - 1} + n(n - 1)(1 - p_n)^{2n - 3} - n^2 (1 - p_n)^{2n - 2} \\
	&= n(1 - p_n)^{n - 1} - n(np_n - p_n + 1)(1 - p_n)^{2n - 2}.
\end{align*}
Therefore, 
\begin{align*}
	\frac{\mathrm{Var}(S_n)}{\mu_n^2} 
	&= \frac{n(1 - p_n)^{n - 1} - n(np_n - p_n + 1)(1 - p_n)^{2n - 2}}{n^2 (1 - p_n)^{2n - 2}} \\
	&= \frac{1}{\mathbb{E}\left[ S_n \right]} - \frac{np_n - p_n + 1}{n} \\
	&\to 0 - 0 \\
	&= 0.
\end{align*}

\subsection*{(c)}
\begin{align*}
	P(S_n = 0) 
	&= P(S_n - \mu_n = \mu_n) \\
	&\leq P(|S_n - \mu_n| \geq \mu_n) \\
	&\leq \frac{\mathrm{Var}(S_n)}{\mu_n} \quad \text{(By Chebyshev's inequality)} \\
	&\to 0.
\end{align*}
Therefore, there exists at least at least one friendless student with probability that converges to 1 as 
$n \to \infty$. In fact, this problem (and the previous problem) demonstrate that $\ln{n}/n$ is an 
\textit{evolution threshold} for isolated vertices for the Erdős–Rényi model.


% Exercise 1.11
\newpage
\section*{Exercise 1.11}
\subsection*{(a)}
First of all, note that the function $f(x) = x^{q/p}$ is convex. By Jensen's inequality, 
\[ \mathbb{E}\left[ |X|^p \right]^{q/p} \leq \mathbb{E}\left[ (|X|^p)^{q/p} \right] 
= \mathbb{E}\left[ |X|^q \right]. \]
Taking $(1/q)$th powers on both sides of the inequality gives the result.

\subsection*{(b)}
First let $q < \infty$, and definte $a := \frac{p + q}{2}$. Let $X$ be the random variable with pdf 
\[ f_X(x) = \frac{a}{x^{a + 1}}, x \geq 1.  \]
Then 
\begin{align*}
	\lVert X \rVert_{L^p}^p
	&= \int_{1}^{\infty} x^p \cdot \frac{a}{x^{a + 1}} \ dx \\
	&= \int_{1}^{\infty} ax^{(p-q)/2 - 1} \ dx \\
	&< \infty \quad (\frac{p - q}{2} - 1 < -1).
\end{align*}
Also, 
\begin{align*}
	\lVert X \rVert_{L^q}^q
	&= \int_{1}^{\infty} x^q \cdot \frac{a}{x^{a + 1}} \ dx \\
	&= \int_{1}^{\infty} ax^{(q-p)/2 - 1} \ dx \\
	&= +\infty \quad (\frac{q - p}{2} - 1 \geq -1).
\end{align*}

By taking the limit for $q \to \infty$, we can also see that the bound is true for when $q = \infty$.


% Exercise 1.12
\newpage
\section*{Exercise 1.12}
We have 
\begin{align*}
	\lVert X \rVert_{L^p}^p 
	&= \mathbb{E}\left[ |X| \cdot |X|^{p - 1} \right] \\
	&= \lVert X \cdot X^{p - 1} \rVert_{L^1} \\
	&\leq \lVert X \rVert_{L^1} \lVert X^{p - 1} \rVert_{L^{\infty}} \quad \text{(Hölder's inequality)} \\
	&= \lVert X \rVert_{L^1} \lVert X \rVert_{L^{\infty}}^{p - 1}.
\end{align*}
Taking both sides to the $1/p$th power gives the result.


% Exercise 1.13
\newpage
\section*{Exercise 1.13}


% Exercise 1.14
\newpage
\section*{Exercise 1.14}
Consider the random vector $X = (X_1, \dots, X_n)$. For the first inequality, 
\begin{align*}
	\left( \sum_{i = 1}^{n} (\mathbb{E}\left[ X_i \right])^p \right)^{1/p} 
	&= \lVert \mathbb{E}\left[ X \right] \rVert_{p} \\
	&\leq \mathbb{E}\left[ \lVert X \rVert_{p} \right] \quad \text{(By Jensen's inequality)} \\
	&= \mathbb{E}\left[ \left( \sum_{i = 1}^{n} X_i^p \right)^{1/p} \right].
\end{align*}
For the second inequality, 
\begin{align*}
	\mathbb{E}\left[ \left( \sum_{i = 1}^{n} X_i^p \right)^{1/p} \right]^p 
	&\leq \mathbb{E}\left[ \left( \sum_{i = 1}^{n} X_i^p \right)^{1/p \cdot p} \right] \\
	&= \mathbb{E}\left[ \sum_{i = 1}^{n} X_i^p \right].
\end{align*}
Taking the $p$th root on both sides of the equation gives the inequality.


% Exercise 1.15
\newpage
\section*{Exercise 1.15}
\subsection*{(a)}
First note that, for every $x \in \mathbb{R}$, we can express $x$ as follows: 
\[ x = \int_{0}^{x} 1 \ dt = \int_{0}^{\infty} \mathbf{1}_{\{ t < x \}} \ dt 
- \int_{-\infty}^{0} \mathbf{1}_{\{ t > x \}} \ dt. \]
Therefore, by plugging the random variable $X$ in and taking expectations on both sides of the equation, we get 
\[ \mathbb{E}\left[ X \right] = \int_{0}^{\infty} P(X > t) \ dt - \int_{-\infty}^{0} P(X < t) \ dt. \]

\subsection*{(b)}
By definition, $f(x)$ has to be nonnegative. Then we can express it as 
\[ f(x) = \int_{0}^{x} f'(t) \ dt = \int_{0}^{\infty} \mathbf{1}_{\{ t < x \}} f'(t) \ dt. \]
Then by plugging in the random variable $X$ and taking expectations on both sides of the equation, we get 
\[ \mathbb{E}\left[ f(X) \right] = \int_{0}^{\infty} P(X > t)f'(t) \ dt. \]

\subsection*{(c)}
Note that $|X|$ is nonnegative, and the function $f: \mathbb{R}_+ \to \mathbb{R}_+$ defined by $f(x) = x^p$ is 
increasing and differentiable for every $p \in (0, \infty)$, and $f(0) = 0$. By applying part (b), 
\[ \mathbb{E}\left[ |X|^p \right] = \int_{0}^{\infty} P(|X| > t)f'(t) \ dt 
= \int_{0}^{\infty} P(|X| > t)pt^{p - 1} \ dt. \]


% Exercise 1.16
\newpage
\section*{Exercise 1.16}
We can rewrite the expectation as follows:
\begin{align*}
	\mathbb{E}\left[ X \right] 
	&= \mathbb{E}\left[ X \mathbf{1}_{\{ X \leq \varepsilon \mathbb{E}\left[ X \right] \}} \right] 
	+ \mathbb{E}\left[ X \mathbf{1}_{\{ X > \varepsilon \mathbb{E}\left[ X \right] \}} \right] \\
	&\leq \varepsilon \mathbb{E}\left[ X \right] + \mathbb{E}\left[ X \right]^{1/2} 
	P(X > \varepsilon \mathbb{E}\left[ X \right])^{1/2} \quad \text{(By Cauchy-Schwartz)}.
\end{align*}
Then, move all terms with $\mathbb{E}\left[ X \right]$ to one side and squaring the inequality gives
\[ (1 - \varepsilon)^2 \mathbb{E}\left[ X \right]^2 \leq \mathbb{E}\left[ X^2 \right] 
P(X > \varepsilon \mathbb{E}\left[ X \right]). \]
Then, dividing $\mathbb{E}\left[ X^2 \right]$ on both sides of the inequality above gives 
\[ P(X > \varepsilon \mathbb{E}\left[ X \right]) \geq (1 - \varepsilon)^2 
\frac{\mathbb{E}\left[ X \right]^2}{\mathbb{E}\left[ X^2 \right]}, \]
hence the result is proven.


% Exercise 1.17
\newpage
\section*{Exercise 1.17}
\subsection*{(a)}
For the first equality, without loss of generality we can assume that $\lVert x \rVert_{p} = 1$. Then we have 
that 
\[ |x_i| \leq 1 \text{ for all } i \implies |x_i|^q \leq |x_i|^p \text{ for all } q \geq p. \]
This gives 
\begin{align*}
	\lVert x \rVert_{q} 
	&= \left( \sum_{i = 1}^{n} |x_i|^q \right)^{1/q} \\
	&\leq \left( \sum_{i = 1}^{n} |x_i|^p \right)^{1/q} \\
	&= \lVert x \rVert_{p}^{p/q}.
\end{align*}
Taking $q$th powers on both sides of the equation gives the first inequality.

For the second inequality, let us denote 
\[ y := (|x_1|^p, \dots, |x_n|^p)m, \ z := \mathbf{1}_{n}. \]
Consider the conjugate exponents $\frac{q}{p}$ and $\frac{q}{q - p}$ (it's pretty easy to check that this 
pair is indeed a pair of conjugate exponents). We have
\begin{align*}
	\lVert x \rVert_{p}^p 
	&= \sum_{i = 1}^{n} |x_i|^p \\
	&= \left\langle y, z \right\rangle \\
	&\leq \left( \sum_{i = 1}^{n} y_i^{q/p} \right)^{p/q} \cdot 
	\left( \sum_{i = 1}^{n} z_i^{q / (q - p)} \right)^{(q - p)/q} \quad \text{(By Hölder's inequality)} \\
	&= \lVert x \rVert_{q}^p \cdot n^{1 - \frac{p}{q}}.
\end{align*}
Taking the $p$th root on both sides of the equation gives 
\[ \lVert x \rVert_{p} \leq n^{\frac{1}{p} - \frac{1}{q}} \lVert x \rVert_{q}, \]
which is exactly what we're looking for.

\subsection*{(b)}
Consider $x = e_i$, the $i$th standard basis vector in $\mathbb{R}^n$. Then 
\[ \lVert x \rVert_{p} = (0^p + \cdots + 0^p + 1^p)^{1/p} = 1 = \lVert x \rVert_{q}. \]
Now consider $y = \mathbf{1}_n$, the vector of all ones in $\mathbb{R}^n$. Then 
\[ \lVert y \rVert_{p} = n^{1/p} = n^{1/p-1/q+1/q} = n^{1/p-1/q}\lVert y \rVert_{q}. \]
Therefore the two bounds in part (a) can both be tight.


% Exercise 1.18
\newpage
\section*{Exercise 1.18}
\subsection*{(a)}
Take $q = \infty$ from Exercise 1.17 to get 
\[ \lVert x \rVert_{\infty} \leq \lVert x \rVert_{p} \leq n^{1/p} \lVert x \rVert_{\infty}. \]
As $p \to \infty$, $n^{1/p} \to 1$ hence $\lVert x \rVert_{p} \to \lVert x \rVert_{\infty}$.

\subsection*{(b)}
Again, take $q = \infty$ to get 
\[ \lVert x \rVert_{\infty} \leq \lVert x \rVert_{p} \leq n^{1/p} \lVert x \rVert_{\infty}. \]
On the right hand side of the inequality, 
\[ n^{1/p} \lVert x \rVert_{\infty} = e^{\ln{n}/p} \lVert x \rVert_{\infty} \leq 
e \lVert x \rVert_{\infty} \text { only if } p > \ln{n}. \]


% Exercise 1.19
\newpage
\section*{Exercise 1.19}
\subsection*{(a)}

