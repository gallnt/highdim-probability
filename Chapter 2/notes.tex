% \title{Notes for Chapter 2: Concentration of Sums of Independent Random Variables}
% \author{Gallant Tsao}

\section*{Why Concentration Inequalities?}
From previous chapters, the simplest concentration inequality is Chebyshev's Inequality, which is quite 
general but the bounds can often can be too weak. We can look at the following example: 
\begin{example}
Toss a fair coin $N$ times. What is the probability that we get at least $\frac{3}{4}$ heads?

Let $S_N$ denote the number of heads, then $S_N \sim \text{Binom}(N, \frac{1}{2})$. We get 
\[ \mathbb{E}[S_N] = \frac{N}{2}, \mathrm{Var}(S_n) = \frac{N}{4}. \]
Using Chebyshev's Inequality, we get 
\[ P(S_N \geq \frac{3}{4}N) \leq P(\bigg| S_N - \frac{N}{2} \bigg| \geq \frac{N}{4}) \leq \frac{4}{N}. \]
This means probabilistic bound from above converges linearly in $N$. 

However, by using the Central Limit Theorem, we get a very different result: If we let $S_N$ be a sum of 
independent $Be(\frac{1}{2})$ random variables. Then by the De Moivre-Laplace CLT, the random variable 
\[ Z_N = \frac{S_N - N/2}{\sqrt{N/4}} \]
converges to the standard normal distribution $N(0, 1)$. Then for a large $N$, 
\[ P(S_ \geq \frac{3}{4}N) = P(Z_N \geq \sqrt{N/4}) \approx P(Z \geq \sqrt{N/4}) \]
where $Z \sim N(0, 1)$. We will use the following proposition: 

\begin{proposition}[Gaussian tails]
Let $Z \sim N(0, 1)$. Then for all $t > 0$, 
\[  \frac{t}{t^2 + 1} \cdot \frac{1}{\sqrt{2 \pi}}e^{-t^2 / 2} \leq P(Z \geq t) 
\leq \frac{1}{t} \cdot \frac{1}{\sqrt{2 \pi}}e^{-t^2 / 2}. \]
\end{proposition}

\begin{proof}
The first inequality is proved in exercise 2.2. For the second inequality, by making the change 
of variables $x = t + y$,
\begin{align*}
	P(Z \geq t) 
	&= \frac{1}{\sqrt{2 \pi}} \int_{t}^{\infty} e^{-x^2 / 2} \ dx \\
	&= \frac{1}{\sqrt{2 \pi}} \int_{0}^{\infty} e^{-t^2 / 2} e^{-ty} e^{-y^2 / 2} \ dy \\
	&\leq \frac{1}{\sqrt{2 \pi}} e^{-t^2 / 2} \int_{0}^{\infty} e^{-ty} \ dy 
	\quad (e^{-y^2 / 2} \leq 1) \\
	&= \frac{1}{t} \cdot \frac{1}{\sqrt{2 \pi}}e^{-t^2 / 2}.
\end{align*}
\end{proof}

Therefore the probability of having at least $\frac{3}{4}N$ heads is bounded by 
\[ \frac{1}{\sqrt{2 \pi}}e^{-N / 8}, \]
which is much better than the linear convergence we had above. However, this reasoning is not rigorous, 
as the approximation error decays slowly, which can be shown via the CLT below: 

\begin{theorem}[Berry-Esseen CLT]
Let $X_1, X_2, \dots$ be a sequence of i.i.d. random variables with mean $\mu$ and variance 
$\sigma^2$, and let $S_N = X_1 + \cdots + X_N$, and let 
\[ Z_N = \frac{S_N - \mathbb{E}[S_N]}{\sqrt{\mathrm{Var}(S_N)}}. \]
Then for every $N \in \mathbb{N}$ and $t \in \mathbb{R}$ we have 
\[ |P(Z_N \geq t) - P(Z \geq t)| \leq \frac{\rho}{\sqrt{N}}, \]
where $Z \sim N(0, 1)$ and $\rho = \mathbb{E}[|X_1 - \mu|^3] / \sigma^3$.
\end{theorem}
Therefore the approximation error decays at a rate of $1 / \sqrt{N}$. Moreover, this bound cannot be improved, 
as for even $N$, the probability of exactly half the flips being heads is 
\[ P(S_N = \frac{N}{2}) = 2^{-N} \binom{N}{N/2} \approx \sqrt{\frac{2}{\pi N}}. \]
where the last approximation uses Stirling approximation.

All in all, we need theory for concentration which bypasses the Central Limit Theorem.
\end{example}

\section*{Hoeffding Inequality}
\begin{definition}[]
A random variable $X$ has the \underline{Rademacher Distribution} if it takes values $-1$ and $1$ 
with probability $1/2$ each, i.e. 
\[ P(X = -1) = P(X = 1) = \frac{1}{2}. \]
\end{definition}

\begin{theorem}[Hoeffding Inequality]
Let $X_1, \sotd, X_N$ be independent Rademacher random variables, and let $a = (a_1, \cdots, a_n) 
\in \mathbb{R}^n$ be fixed. Then for any $t \geq 0$, 
\[ P \biggl( \sum_{i = 1}^{N} a_iX_i \geq t \biggr) \leq \exp{\biggl( -\frac{t^2}{2\|a\|_2^2} \biggr)}. \]
\end{theorem}

\begin{proof}
The proof comes by a method called the \textit{exponential moment method}. We multiply the probability of 
the quantity of interest by $\lambda \geq 0$ (whose value will be determined later), exponentiate, 
and then bound using Markov's inequality, which gives: 
\begin{align*}
	P \biggl( \sum_{i = 1}^{N} a_iX_i \geq t \biggr) 
	&= P \biggl( \lambda \sum_{i = 1}^{N} a_iX_i \geq \lambda t \biggr) \\
	&= P \biggl( \exp{\biggl( \lambda \sum_{i = 1}^{N} a_iX_i \biggr)} \geq \exp{(\lambda t)} \biggr) \\
	&\leq e^{-\lambda t} \mathbb{E} \biggl[ \exp{\biggl( \lambda \sum_{i = 1}^{N} a_iX_i \biggr)} \biggr].
\end{align*}
In fact, from the last quantity we got above, we are effectively trying to bound the moment generating 
function of the sum $\sum_{i = 1}^{N} a_iX_i$. Since the $X_i$'s are independent, 
\[ \mathbb{E} \biggl[ \exp{\biggl( \lambda \sum_{i = 1}^{N} a_iX_i \biggr)} \biggr] 
= \prod_{i = 1}^{N} \mathbb{E}[\exp{(\lambda a_i X_i)}]. \]
Let's fix $i$. Since $X_i$ takes values $-1$ and $1$ with probability $1/2$ each, 
\[ \mathbb{E}[\exp{(\lambda a_i X_i)}] = \frac{1}{2}\exp{(\lambda a_i)} 
+ \frac{1}{2}\exp{(-\lambda a_i)} = \cosh{(\lambda a_i)}. \]
Next we will use the following inequality: 
\[ \cosh{x} \leq e^{x^2/2} \quad \text{for all } x \in \mathbb{R}. \]
The above is true by expanding the taylor series for both functions (proven in Exercise 2.5). Then 
we get 
\[ \mathbb{E}[\exp{(\lambda a_i X_i)}] \leq \exp{(\lambda^2 a_i^2 / 2)}. \]
Substituting this inequality into what we have above gives 
\begin{align*}
	P \biggl( \sum_{i = 1}^{N} a_iX_i \geq t \biggr) 
	&\leq e^{-\lambda t} \prod_{i = 1}^{N} \exp{(\lambda^2 a_i^2 / 2)} \\
	&= \exp{\biggl( -\lambda t + \frac{\lambda^2}{2}\sum_{i = 1}^{N} a_i^2 \biggr)} \\
	&= \exp{\biggl( -\lambda t + \frac{\lambda^2}{2} \|a\|_2^2 \biggr)}.
\end{align*}
Now we want to find the optimal value of $\lambda$ to make the quantity on the RHS as small as possible. 
Define the RHS as a function of $\lambda$,  and taking derivatives with respect to $\lambda$ yields
\[ f'(\lambda) = (-t + \lambda \|a\|_2^2) 
\exp{\biggl( -\lambda t + \frac{\lambda^2}{2} \|a\|_2^2 \biggr)} = 0 
\implies \lambda^* = \frac{t}{\|a\|_2^2}. \]
Then the second derivative test gives 
\[ f''(\lambda^*) = \|a\|_2^2 \exp{\biggl( -\lambda t + \frac{\lambda^2}{2} \|a\|_2^2 \biggr)} \geq 0. \]
Therefore the quantity is indeed minimized at $\lambda^*$, then plugging this value back gives 
\[ P \biggl( \sum_{i = 1}^{N} a_iX_i \geq t \biggr) 
\leq \exp{\biggl( -\frac{t^2}{2\|a\|_2^2} \biggr)}. \]


\end{proof}
