\section{Sums of Independent Random Variables}

% Exercise 2.1
\section*{Exercise 2.1}
First note that by independence, 
\[ \mathbb{E}[Y_n] = \mathbb{E}[X_1] \cdots \mathbb{E}[X_n] = 0.5^n. \]
For the first inequality, denote $A = \{X_i \geq 0.5 \quad \forall i\}$. Then we can get that 
$P(A) \leq P(Y_n \geq \mathbb{E}[Y_n])$ because $A \subseteq \{Y_n  \geq \mathbb{E}[Y_n]\}$. By independence, 
$P(A) = 0.5^n$ and we are done. For the second inequality, since all $X_i$ are nonnegative, 
by Markov's inequality we get 
\[ P(Y_n \geq \mathbb{E}[Y_n]) = P(\sqrt{Y_n} \geq \sqrt{\mathbb{E}[Y_n]}) 
\leq \frac{\mathbb{E}[\sqrt{Y_n}]}{\sqrt{\mathbb{E}[Y_n]}}. \]
The denominator is just $\sqrt{(1/2)^n}$ from earlier. For the numerator, since $X_i$ are iid, 
\[ \mathbb{E}[\sqrt{Y_n}] = (\mathbb{E}[\sqrt{X_i}])^n. \]
Now this problem amounts to finding the expected value of $\sqrt{X_i}$.
\begin{align*}
	F_{\sqrt{X_i}}(x) 
	&= P(\sqrt{X_i} \leq x) \\
	&= P(X_i \leq x^2) \\
	&= F_{X_i}(x^2) \\
	&= x^2.
\end{align*}
Then the pdf of $\sqrt{X_i}$ is 
\[ f_{\sqrt{X_i}}(x) = 2x, \quad x \in [0, 1]. \]
Finally, we get that 
\[ \mathbb{E}[\sqrt{X_i}] = \int_{0}^{1} 2x^2 \ dx = \frac{2}{3} 
\implies P(Y_n \geq \mathbb{E}[Y_n]) \leq \frac{(2/3)^n}{(1/2)^{n/2}} 
= \biggl( \frac{2 \sqrt{2}}{3} \biggr)^n \leq 0.95^n. \]


% Exercise 2.2
\newpage
\section*{Exercise 2.2}
We'll approach this problem via a more calculus-based flavor.
Define the function 
\begin{align*}
	f(t) 
	&:= P(g \geq t) - \frac{t}{t^2 + 1} \cdot \frac{1}{\sqrt{2 \pi}} e^{-t^2 / 2} \\
	&= \int_{t}^{\infty} \frac{1}{\sqrt{2 \pi}} e^{-x^2 / 2} \ dx.
\end{align*}
First we can immediately see that $f(x) = \frac{1}{2} > 0$. Moreover, 
\begin{align*}
	f'(t) 
	&= \frac{1}{\sqrt{2 \pi}}e^{-t^2 / 2} + \frac{t^4 + 2t^2 - 1}{(t^2 + 1)^2} 
	\cdot \frac{1}{\sqrt{2 \pi}} e^{-t^2 / 2} \\
	&= -\frac{2}{(t^2 + 1)^2} \cdot \frac{1}{\sqrt{2 \pi}} e^{-t^2 / 2} \\
	&< 0 \quad \text{ for all } t > 0.
\end{align*}
We can also see that 
\[ \lim_{t \to \infty} f(t) = 0 - 0 = 0. \]
From these three facts, $f(t) \geq 0$ for all $t > 0$ hence the inequality follows.


% Exercise 2.3
\newpage
\section*{Exercise 2.3}
\subsection*{(a)}
\[ f'(x) = (-x) \cdot \frac{1}{\sqrt{2 \pi}} e^{-x^2 / 2} = -xf(x) \text{ for all } x. \]

\subsection*{(b)}
Using integration by parts and the property from part (a), 
\begin{align*}
	\int_{t}^{\infty} f(x) \ dx
	&= \int_{t}^{\infty} -\frac{f'(x)}{x} \ dx \\
	&= \biggl[ -\frac{f(x)}{x} \biggr]_t^{\infty} - \int_{t}^{\infty} f(x) \ d(-\frac{1}{x}) \\
	&= \frac{f(t)}{t} - \int_{t}^{\infty} \frac{f(x)}{x^2} \ dx \\
	&= \frac{f(t)}{t} - \int_{t}^{\infty} -\frac{f'(x)}{x^3} \ dx \\
	&= \frac{f(t)}{t} - \biggl[ \biggl[ -\frac{f(x)}{x^3} \biggr]_t^\infty 
	- \int_{t}^{\infty} f(x) \ d(-\frac{1}{x^3}) \biggr] \\
	&= \frac{f(t)}{t} - \frac{f(t)}{t^3} + 3 \int_{t}^{\infty} \frac{f(x)}{x^4} \ dx \quad 
	{\color{blue} (*)} \\
	&= \frac{f(t)}{t} - \frac{f(t)}{t^3} + 3 \int_{t}^{\infty} -\frac{f'(x)}{x^5} \ dx \\
	&= \frac{f(t)}{t} - \frac{f(t)}{t^3} + 3 \biggl[ -\frac{f(x)}{x^5} \biggr]_t^\infty 
	- \int_{t}^{\infty} f(x) \ d(-\frac{1}{x^5}) \\
	&= \frac{f(t)}{t} - \frac{f(t)}{t^3} + \frac{3f(t)}{t^5} - 5 \int_{t}^{\infty} \frac{f(x)}{x^6} \ dx 
	. \quad {\color{blue} (**)}
\end{align*}
Since $f(t) > 0$, dividing $\color{blue} (*)$ by $f(t)$ we will get the first inequality, and dividing 
$\color{blue} (**)$ by $f(t)$ results in the second inequality.


% Exercise 2.4
\newpage
\section*{Exercise 2.4}
\subsection*{(a)}
\begin{align*}
	\mathbb{E}[g \mathbf{1}_{g > t}] 
	&= \int_{t}^{\infty} x \cdot \frac{1}{\sqrt{2 \pi}} e^{-x^2 / 2} \ dx \\
	&= \biggl[ -\frac{1}{\sqrt{2 \pi}} e^{-x^2 / 2} \biggr]_t^\infty \\
	&= \frac{1}{\sqrt{2 \pi}} e^{-t^2 / 2}.
\end{align*}

\subsection*{(b)}
\begin{align*}
	\mathbb{E}[g^2 \mathbf{1}_{g > t}] 
	&= \int_{t}^{\infty} x^2 \cdot \frac{1}{\sqrt{2 \pi}} e^{-x^2 / 2} \ dx \\
	&= \biggl[ -\frac{x}{\sqrt{2 \pi}} e^{-x^2 / 2} \biggr]_t^\infty 
	+ \int_{t}^{\infty} \frac{1}{\sqrt{2 \pi}} e^{-x^2 / 2} \ dt \\
	&\leq \frac{t}{\sqrt{2 \pi}} e^{-t^2 / 2} + \frac{1}{t} \cdot 
	\frac{1}{\sqrt{2 \pi}} e^{-t^2 / 2} \quad \text{(prop 2.1.2)} \\
	&= \biggl( t + \frac{1}{t} \biggr) \cdot \frac{1}{\sqrt{2 \pi}} e^{-t^2 / 2}.
\end{align*}


% Exercise 2.5
\newpage
\section*{Exercise 2.5}
We start by expanding both quantities into their respective Taylor series representations: 
\begin{align*}
	\cosh{x} 
	&= \frac{e^x + e^{-x}}{2} \\
	&= \frac{1}{2}\sum_{n = 0}^{\infty} \frac{x^n}{n!} + \frac{1}{2}\sum_{n = 0}^{\infty} 
	(-1)^n \frac{x^n}{n!} \\
	&= \sum_{n = 0}^{\infty} \frac{x^{2n}}{(2n)!} \quad (x \in \mathbb{R}),
\end{align*}
\[ \exp{(x^2 / 2)} = \sum_{n = 0}^{\infty} \frac{(x^2 / 2)^n}{n!} 
= \sum_{n = 0}^{\infty} \frac{x^{2n}}{n! 2^n} \quad (x \in \mathbb{R}). \]
From the above, if we subtract the Taylor series of $\cosh{x}$ from that of $\exp{(x^2 / 2)}$, to show 
that this quantity is nonnegative, it is enough to show that 
\[ n! 2^n \leq (2n)! \quad \forall n \in \mathbb{N}_0. \]
We'll proceed via proof by induction. For the base case, when $n = 0$, 
\[ 0! 2^0 = 1 \leq 1 = (2 \cdot 0)! \]
For the inductive step, assume that for some $n \in \mathbb{N}_0$, the statement is true.
\begin{align*}
	(n + 1)! 2^{n + 1} 
	&= 2(n + 1) n! 2^n \\
	&\leq 2(n + 1) \cdot (2n)! \\
	&\leq (2n + 2)(2n + 1)(2n)! \\
	&= (2(n + 1))!
\end{align*}
Therefore we are done.


% Exercise 2.6
\newpage
\section*{Exercise 2.6}
As per usual, let $\lambda > 0$. By the typical procedure for the exponential moment method, 
\begin{align*}
	P(g \geq t) 
	&= P(\lambda g \geq \lambda t) \\
	&= P(\exp{(\lambda g)} \geq \exp{(\lambda t)}) \\
	&\leq e^{-\lambda t} \mathbb{E}[\exp{(\lambda g)}] \\
	&= \exp{(-\lambda t + \lambda^2 / 2)}.
\end{align*}
Defining the quantity above as a function $f(\lambda)$, 
\[ f'(\lambda) = (-t + \lambda)\exp{(-\lambda t + \lambda^2 / 2)} = 0 \implies \lambda^* = t. \]
Moreover, 
\[ f''(\lambda^*) = \exp{(-\lambda t + \lambda^2 / 2)} > 0. \]
Therefore by the second derive test, we have found a minimizer $\lambda^* = t$ for the quantity on the 
RHS bound. Plugging back gives the result.


% Exercise 2.7
\newpage
\section*{Exercise 2.7}
Again, we'll use the exponential moment method so let $\lambda > 0$. 
\begin{align*}
	P \biggl( \sum_{i = 1}^{N} X_i \leq \varepsilon N \biggr) 
	&= P \biggl( \sum_{i = 1}^{N} -(X_i / \varepsilon) \geq -N \biggr) \\
	&= P \biggl( \exp{\biggl( \sum_{i = 1}^{N} -(X_i / \varepsilon)} \geq 
	\exp{(- \lambda N)} \biggr) \\
	&\leq e^{\lambda N} \mathbb{E}[\exp{\biggl( \lambda \sum_{i = 1}^{n} -X_i / \varepsilon \biggr)}] \\
	&= e^{\lambda N} \prod_{i = 1}^{N} \mathbb{E}[\exp{(-\lambda X_i / \varepsilon)}].
\end{align*}
Now fix $i$. Since $X_i$ is uniformly bounded by $K$, we have 
\begin{align*}
	\mathbb{E}[\exp{(-\lambda X_i / \varepsilon)}] 
	&= \int_{0}^{\infty} e^{-\lambda x / \varepsilon} f_X(x) \ dx \\
	&\leq \int_{0}^{\infty} Ke^{-\lambda x / \varepsilon} \ dx \\
	&= K \biggl[ -\frac{\varepsilon}{\lambda} e^{-\lambda x / \varepsilon} \biggr]_0^\infty \\
	&= \frac{K \varepsilon}{\lambda}.
\end{align*}
Combining the above gives 
\[ P \biggl( \sum_{i = 1}^{N} X_i  \leq \varepsilon N \biggr) \leq e^{\lambda N} 
\biggl( \frac{K \varepsilon}{\lambda} \biggr)^N = (K \varepsilon)^N e^{\lambda N} \lambda^{-N}. \]
Defining the result above as $f(\lambda)$ and differentiate, we get 
\[ f'(\lambda) = (K \varepsilon)^N (N \lambda - N) e^{\lambda N} \lambda^{-N - 1} = 0 
\implies \lambda^* = 1. \]
Moreover, 
\[ f''(\lambda^*) = (K \varepsilon)^N \cdot Ne^{\lambda N} > 0. \]
Therefore $\lambda^* = 1$ is the minimizer of our bound, and plugging back gives the result.


% Exercise 2.8
\newpage
\section*{Exercise 2.8}
The function $f(x) = e^{\lambda x}$ is convex because $f''(x) = \lambda^2 e^{\lambda x} \geq 0$. By 
Jensen's inequality, for any $a, b \in \mathbb{R}$ and $p \in [0, 1]$, 
\begin{align*}
	f(pa + (1 - p)b) 
	&= e^{\lambda(pa + (1 - p)b)} \\
	&\leq p e^{\lambda a} + (1 - p) e^{\lambda b} \\
	&= pf(a) + (1 - p)f(b).
\end{align*}
In particular, this means for any $x \in [a, b]$, 
\[ e^{\lambda x} \leq e^{\lambda a} + (1 - p) e^{\lambda b} 
\implies e^{\lambda X} \leq e^{\lambda a} + (1 - p) e^{\lambda b}. \]
Taking expectations on both sides, 
\[ \mathbb{E}[e^{\lambda X}] \leq e^{\lambda a} + (1 - p) e^{\lambda b} = \mathbb{E}[e^{\lambda Y}]. \]


% Exercise 2.9
\newpage
\section*{Exercise 2.9}
\subsection*{(a)}
We can assume WLOG $X$ has mean zero because we can define another random variable $Y = X - \mathbb{E}[X]$, 
then $Y$ will take values between $[a - \mathbb{E}[X], b - \mathbb{E}[X]]$, which does not affect the 
analysis. 

We can assume WLOG $b - a = 1$ because we can define another random variable $Y = X / (b - a)$. Then 
Y takes values in $[a / (b - a), b / (b - a)]$, which again does not affect the analysis.

We can assume that $X$ takes values in $\{a, b\}$ because from Exercise 2.8 (add link), if we prove that 
the bound is true for the discrete version, we have also effectively proven it for the continuous version.

\subsection*{(b)}
Without loss of generality assume $X$ satisfies everything in part (a). Define $P(X = a) = p$. Then 
from the expectation and range conditions, 
\[ \begin{cases}
	pa + (1 - p)b &= 0 \\
	b - a &= 1
\end{cases} \implies p = a + 1 = b, \ 1 - p = -a. \]
After finding $p$, we can solve for the cumulant generating function $K(\lambda)$: 
\begin{align*}
	K(\lambda) 
	&= \ln{(\mathbb{E}[e^{\lambda X}])} \\
	&= \ln{(be^{\lambda a} + (-a)e^{\lambda b})} \\
	&= \ln{(be^{\lambda a} - ae^{\lambda(a + 1)})} \\
	&= \ln{e^{\lambda a}(b - ae^\lambda)} \\
	&= \lambda a + \ln{(b - ae^\lambda)}.
\end{align*}
We can see clearly that $K(0) = 0 + \ln{(b - a)} = 0$. We also get 
\[ K'(\lambda) = a - \frac{ae^\lambda}{b - ae^\lambda} \implies K'(0) = a - a = 0. \]
Moreover, 
\begin{align*}
	K''(\lambda) 
	&= -\frac{ae^\lambda (b - ae^\lambda) - ae^\lambda \cdot (-ae^\lambda)}{(b - ae^\lambda)^2} \\
	&= -\frac{abe^\lambda}{(b - ae^\lambda)^2} \\
	&\leq \frac{(-ae^\lambda + b)^2}{2^2 (b - ae^\lambda)^2} \quad \text{(AM-GM inequality with } 
	x = -ae^\lambda, y = b) \\
	&= \frac{1}{4}.
\end{align*}
Then by Taylor's Theorem, 
\[ K(\lambda) 
= K(0) + \lambda K'(0) + \frac{\lambda^2 K''(\xi)}{2!} 
\leq 0 + 0 + \frac{\lambda^2}{4 \cdot 2!} = \frac{\lambda^2}{8} \text{ for all } \lambda \in \mathbb{R}. \]


% Exercise 2.10
\newpage
\section*{Exercise 2.10}
We're going to use the exponential moment method. Let $\lambda > 0$. Then 
\begin{align*}
	P \left( \sum_{i = 1}^{n} (X_i - \mathbb{E}\left[ X_i \right]) \geq t \right) 
	&= P \left( \exp{\left( \lambda \sum_{i = }^{N}(X_i - \mathbb{E}\left[ X_i \right]) \right)} 
	\geq \exp{(\lambda t)} \right) \\
	&\leq e^{-\lambda t} \prod_{i = 1}^{N} \mathbb{E}\left[ \exp{(\lambda X_i - \mathbb{E}\left[ X_i
	\right])} \right] \quad \text{(By Markov + independence)} \\
	&= e^{-\lambda t} \prod_{i = 1}^{N} \exp{\left( \frac{\lambda^2 (b_i - a_i)^2}{8} \right)} 
	\quad \text{(By Exercise 2.9)} \\
	&= \exp{\left( -\lambda t + \frac{\lambda^2}{8}\sum_{i = 1}^{N} (b_i - a_i)^2 \right)}.
\end{align*}
Again, we set the right hand side to be a function of $\lambda$ and minimize it. 
\[ f'(\lambda) = \left( -t + \frac{\lambda}{4}\sum_{i = 1}^{N}(b_i - 1_i)^2 \right)f(\lambda) = 0 
\implies \lambda^* = \frac{4t}{\sum_{i = 1}^{N}(b_i - a_i)^2}. \]
Moreover, 
\begin{align*}
	f''(\lambda) = \frac{1}{4}\sum_{i = 1}^{N}(b_i - a_i)^2 f(\lambda) 
	+ \left( -t + \frac{\lambda}{4}\sum_{i = 1}^{N}(b_i - a_i)^2 \right) f(\lambda) \\
	\implies f''(\lambda^*) = \frac{1}{4}\sum_{i = 1}^{N}(b_i - a_i)^2 \cdot 
	\exp{\left( -\frac{2t^2}{\sum_{i = 1}^{N}(b_i - a_i)^2} \right)} \geq 0. 
\end{align*}
By the second derivative test, $\lambda*$ indeed minimizes the expression that we found earlier. Lastly, plug 
$\lambda^*$ back to the original equation above gives the result.


% Exercise 2.11
\newpage
\section*{Exercise 2.11}
We'll use the exponential moment method. Let $\lambda > 0$. Then 
\begin{align*}
	P(S_N \leq t) 
	&= P(-S_N \geq -t) \\
	&= P(\exp{(-\lambda S_N)} \geq \exp{(-\lambda t)}) \\
	&\leq e^{\lambda t} \mathbb{E}\left[ \exp{(-\lambda S_N)} \right] \quad \text{(By Markov's inequality)} \\
	&= e^{\lambda t} \prod_{i = 1}^{N}\mathbb{E}\left[ \exp{(-\lambda X_i)} \right] \quad 
	\text{(By independence)} \\
	&\leq \exp{(\lambda t + (e^{-\lambda} - 1) \mu)}, \quad (*)
\end{align*}
where for the last inequality we used the definition of expectation:
\[ \mathbb{E}\left[ \exp{(-\lambda X_i)} \right] 
= e^{-\lambda}p_i + 1 \cdot (1 - p_i) = 1 + (e^{-\lambda} - 1)p_i \leq \exp{((e^{-\lambda} - 1)p_i)}. \]
Then we get 
\[ \prod_{i = 1}^{N}\mathbb{E}\left[ \exp{(-\lambda X_i)} \right] 
\leq \exp{\left( (e^{-\lambda} - 1) \sum_{i = 1}^{N} p_i \right)} = \exp{((e^{-\lambda} - 1) \mu)}. \]
We set the right hand side of $(*)$ as a function of $\lambda$ and minimize it.
\[ f'(\lambda) = (t - \mu e^{-\lambda})f(\lambda) = 0 \implies e^{-\lambda} = \frac{t}{\mu} 
\implies \lambda^* = \ln{\left( \frac{\mu}{t} \right)}. \]
Moreover, 
\[ f''(\lambda) = \mu e^{-\lambda} f(\lambda) + (t - \mu e^{-\lambda})^2 f(\lambda) 
\implies f''(\lambda^*) = t \cdot \exp{\left( t \ln{\left( \frac{\mu}{t} \right)} + 
\left( \frac{t}{\mu} - 1 \right) \mu \right)} > 0. \]
By the second derivative test, $\lambda^*$ indeed minimizes $(*)$. Plugging $\lambda^*$ back to $(*)$ then 
gives the result.


% Exercise 2.12
\newpage
\section*{Exercise 2.12}
From the pmf of a binomial random variable, 
\begin{align*}
	P(S_N = t) 
	&= \binom{N}{t} \left( \frac{\mu}{N} \right)^t \left( 1 - \frac{\mu}{N} \right)^{N - t} \\
	&\geq \left( \frac{N}{t} \right)^t \left( \frac{\mu}{N} \right)^t 
	\left( 1 - \frac{\mu}{N} \right)^{N - t} \quad \text{(Exercise 0.6)} \\
	&\geq \left( \frac{N}{t} \right)^t \left( \frac{\mu}{N} \right)^t  
    \left( 1 - \frac{\mu}{N} \right)^{N - \mu} \quad (\mu \leq t) \\
	&\geq \left( \frac{N}{t} \right)^t \left( \frac{\mu}{N} \right)^t e^{-\mu} \\
	&= e^{-\mu} \left( \frac{\mu}{t} \right)^t.
\end{align*}


% Exercise 2.13
\newpage
\section*{Exercise 2.13}
\subsection*{(a)}
We'll use the exponential moment method. Let $\lambda > 0$. Then 
\begin{align*}
	P(X \geq t) 
	&= P(e^{\lambda X} \geq e^{\lambda t}) \\
	&\leq e^{-\lambda t}\mathbb{E}\left[ e^{\lambda X} \right] \quad \text{(By Markov's inequality)} \\
	&= \exp{(-\lambda t + \mu(e^{\lambda} - 1))}. \quad (*)
\end{align*}
We set the right hand side of $(*)$ as a function of $\lambda$ and minimize it.
\[ f'(\lambda) = (-t + \mu e^{\lambda}) f(\lambda) = 0 \implies \lambda^* = \ln{\left( \frac{t}{\mu} \right)}. \]
Moreover, 
\[ f''(\lambda) = \mu e^{\lambda} f(\lambda) + (-t + \mu e^\lambda)^2 f(\lambda) 
\implies f''(\lambda^*) = t \exp{(-t \ln{(t/\mu)} + t - \mu)} \geq 0. \]
By the second derivative test, $\lambda^*$ indeed minimizes $(*)$. Plugging $\lambda^*$ back to $(*)$ then 
gives the result.

\subsection*{(b)}
We'll use the exponential moment method. Let $\lambda > 0$. Then 
\begin{align*}
	P(X \leq t) 
	&= P(-X \geq -t) \\
	&= P(e^{-\lambda X} \geq e^{-\lambda t}) \\
	&\leq e^{\lambda t} \mathbb{E}\left[ e^{-\lambda X} \right] \quad \text{(By Markov's inequality)} \\
	&= \exp{(\lambda t + \mu(e^{-\lambda} - 1))}.
\end{align*}
We set the right hand side of $(*)$ as a function of $\lambda$ and minimize it.
\[ f'(\lambda) = (t - \mu e^{-\lambda}) f(\lambda) = 0 \implies \lambda^* = \ln{\left( \frac{\mu}{t} \right)}. \]
Moreover, 
\[ f''(\lambda) = \mu e^{-\lambda} f(\lambda) + (t - \mu e^{-\lambda})^2 f(\lambda) 
\implies f''(\lambda^*) = t \cdot \exp{(t \ln{(\mu / t)} + \mu (t/\mu - 1))} \geq 0. \]
By the second derivative test, $\lambda^*$ indeed minimizes $(*)$. Plugging $\lambda^*$ back to $(*)$ then 
gives the result.

\subsection*{(c)}
By Theorem 2.3.1 with $t = (1 + \delta)\mu$, 
\begin{align*}
	P(X - \mu \geq \delta \mu) 
	&= P(X \geq (1 + \delta)\mu) \\
	&\leq e^{-\mu} \left( \frac{e \mu}{(1 + \delta)\mu} \right)^{(1 + \delta)\mu} 
	\quad \text{(By Chernoff's inequality)} \\
	&= e^{-\mu + (1 + \delta)\mu} \cdot e^{-(1 + \delta)\ln{(1 + \delta)} \mu} \\
	&= \exp{(-\mu((1 + \delta)\ln{(1 + \delta)} - \delta))}.
\end{align*}
From the proof of Corollary 2.3.4, we already proved that 
\[ (1 + \delta)\ln{(1 + \delta)} - \delta \geq \frac{\delta^2}{3}. \]
Therefore, by plugging this into our original bound, we get 
\[ P(X - \mu \geq \delta \mu) \leq \exp{\left( -\frac{\delta^2 \mu}{3} \right)}. \]

For the other side, we apply Remark 2.3.2 and get 
\begin{align*}
	P(X - \mu \leq -\delta \mu) 
	&= P(X \leq (1 - \delta)\mu) \\
	&\leq e^{-\mu} \left( \frac{e \muu}{(1 - \delta)\mu} \right)^{(1 - \delta)\mu} 
	\quad \text{(By Remark 2.3.2)} \\
	&= \exp{(-\mu((1 - \delta)\ln{(1 - \delta)} + \delta))}.
\end{align*}
Again, from the proof of Corollary 2.3.4, we already have 
\[ (1 - \delta)\ln{(1 - \delta)} + \delta \geq \frac{\delta^2}{2} \geq \frac{\delta^2}{3}, \]
hence plugging this bound into our original bound gives
\[ P(X - \mu \leq -\delta \mu) \leq \exp{\left( -\frac{\delta^2 \mu}{3} \right)}. \]
Combining the two results above gives the statement.

\subsection*{(d)}
\[ P(X = t) = \frac{e^{-\mu} \mu^t}{t!} \geq \frac{e^{-\mu} \mu^t}{t^t} 
= e^{-\mu} \left( \frac{\mu}{t} \right)^t. \]


% Exercise 2.14
\newpage
\section*{Exercise 2.14}
For this question, we'll use the following identity:
\[ \ln{(1 + x)} \geq \frac{x}{1 + x/2} \text{ for all } x \geq 0. \]
This is above is true since we can define the function
\[ g(x) := \ln{(1 + x)} - \frac{x}{1 + x/2}. \]
Then 
\[ g(0) = 0, \ g'(x) = \frac{1}{1 + x} - \frac{1}{(1 + x/2)^2} \geq 0. \]
Ok great! Let's go back to the problem. Just like Exercise 2.13, we have to show two sides of the bound, then 
combine together.

For the upper bound, again by applying Theorem 2.3.1 with $t = (1 + \delta)\mu$, 
\begin{align*}
	P(S_N - \mu \geq \delta \mu) 
	&= P(S_N \geq (1 + \delta)\mu) \\
	&\leq \exp{(-\mu((1 + \delta)\ln{(1 + \delta)} - \delta))} \\
	&\leq \exp{\left( -\frac{\delta^2 \mu}{2 + \delta} \right)}
\end{align*}
where in the last inequality, we used that fact that (due to the identity at the beginning),
\begin{align*}
	(1 + \delta)\ln{(1 + \delta)} - \delta 
	&\geq \frac{(1 + \delta)\delta}{1 + \delta/2} - \delta \\
	= \frac{\delta^2 / 2}{1 + \delta/2} \\
	&= \frac{\delta^2}{2 + \delta}.
\end{align*}

For the lower bound, the case where $\delta > 1$ is trivial. For the other case where $\delta \leq 1$, from 
the proof of Corollary 2.3.4, just like the previous exercise, 
\begin{align*}
	P(X - \mu \leq -\delta \mu) 
	&= P(X \leq (1 - \delta)\mu) \\
	&\leq e^{-\mu} \left( \frac{e \muu}{(1 - \delta)\mu} \right)^{(1 - \delta)\mu} 
	\quad \text{(By Remark 2.3.2)} \\
	&= \exp{(-\mu((1 - \delta)\ln{(1 - \delta)} + \delta))}.
\end{align*}
Moreover, we have
\[ (1 - \delta)\ln{(1 - \delta)} + \delta \geq \frac{\delta^2}{2} \geq \frac{\delta^2}{2 + \delta}, \]
and therefore 
\[ P(X - \mu \leq -\delta \mu) \leq \exp{\left( -\frac{\delta^2 \mu}{2 + \delta} \right)}. \]
Combining the two bounds completes the proof.


% Exercise 2.15
\newpage
\section*{Exercise 2.15}
When using the exponential moment method in proving the Chernoff inequalities, the key chain of inequalities 
that we relied on is as follows:
\begin{align*}
	P(S_N \geq t) 
	&\leq P(\exp{(\lambda S_N)} \geq e^{\lambda t}) \\
	&\leq e^{-\lambda t} \mathbb{E}\left[ \exp{(\lambda S_n)} \right] \\
	&= e^{\lambda t} \prod_{i = 1}^{N} \mathbb{E}\left[ \exp{(\lambda X_i)} \right].
\end{align*}
From Exercise 2.8, if $X, Y$ are random variables such that $X$ takes values in $[a, b]$ and $Y$ takes values 
in $\{ a, b \}$, then 
\[ \mathbb{E}\left[ \exp{(\lambda X_i)} \right] \leq \mathbb{E}\left[ \exp{(\lambda Y_i)} \right] 
\text{ for all } \lambda \in \mathbb{R}. \]
Therefore if the bound holds true for independent Bernoulli random variables, they have to hold for any 
independent random variables taking values on $[0, 1]$.


% Exercise 2.16
\newpage
\section*{Exercise 2.16}
Now 


% Exercise 2.21
\newpage
\section*{Exercise 2.21}
Let $X_i = \{ \text{Algorithm at ith time is wrong} \}$. Then $X_i \sim \mathrm{Ber}(\frac{1}{2} - 
\varepsilon)$. By Hoeffding's inequality (Theorem 2.2.6), the probability that the answer is wrong is 
\begin{align*}
	P \left( \sum_{i = 1}^{N} X_i \geq \frac{N}{2} \right) 
	&= P \left( \sum_{i = 1}^{N} X_i - \left( \frac{N}{2} - N \varepsilon \right) \geq \frac{N}{2} - 
	\left( \frac{N}{2} - N \varepsilon \right) \right) \\
	&= P \left( \sum_{i = 1}^{N}\left( X_i - \left( \frac{1}{2} - \varepsilon \right) \right) \geq 
	N \varepsilon \right) \\
	&\leq \exp{\left( -\frac{2N^2 \varepsilon^2}{N} \right)} \\
	&= \exp{(-2N \varepsilon^2)} \\
	&\leq \delta.
\end{align*}
Solving the inequality above for $N$ gives 
\[ -2N \varepsilon^2 \leq \ln{\delta} \implies N \geq \frac{1}{2 \varepsilon^2}\ln{\left( \frac{1}{\delta} 
\right)}. \]


% Exercise 2.22
\newpage
\section*{Exercise 2.22}
\subsection*{(a)}
The pdf of the standard normal distribution is 
\[ f_g(s) = \frac{1}{\sqrt{2 \pi}} e^{-x^2 / 2}, \ x \in \mathbb{R}. \]
Then 
\begin{align*}
	\mathbb{E}\left[ |g|^p \right] 
	&= \int_{-\infty}^{\infty} |x|^p \cdot \frac{1}{\sqrt{2 \pi}} e^{-x^2 / 2} \ dx \\
	&= \int_{-\infty}^{0} (-x)^p \cdot \frac{1}{\sqrt{2 \pi}} e^{-x^2 / 2} \ dx 
	+ \int_{0}^{\infty} x^p \cdot \frac{1}{\sqrt{2 \pi}} e^{-x^2 / 2} \ dx \\
	&= 2 \int_{0}^{\infty} x^p \cdot \frac{1}{\sqrt{2 \pi}} e^{-x^2 / 2} \ dx \\
	&= 2 \int_{0}^{\infty} (2t)^{(p - 1)/2} \cdot \frac{1}{\sqrt{2 \pi}} e^{-t} \ dt \quad (t = x^2/2) \\
	&= \frac{2^{p/2}}{\sqrt{\pi}} \int_{0}^{\infty} t^{(p - 1)/2} e^{-t} \ dt \\
	&= \frac{2^{p/2}}{\sqrt{\pi}} \Gamma \left( \frac{p + 1}{2} \right).
\end{align*}

\subsection*{(b)}
From part (a), 
\begin{align*}
	\lVert g \rVert_{L^p} 
	&= \left[ \frac{2^{p/2}}{\sqrt{\pi}} \Gamma \left( \frac{p + 1}{2} \right) \right]^{1/p} \\
	&\approx \left[ \frac{2^{p/2}}{\sqrt{\pi}} \cdot \sqrt{2 \pi(p + 1) / 2} \cdot 
	\left( \frac{p - 1}{2e} \right)^{(p - 1)/2} \right]^{1/p} \\
	&= \left[ \sqrt{2(p - 1)} \cdot \left( \frac{p - 1}{e} \right)^{(p-1)/2} \right]^{1/p} \\
	&= \left[ \sqrt{2e} \left( \frac{p}{e} \right)^{p/2} \right]^{1/p} \\
	&= \sqrt{\frac{p}{e}} \cdot (2e)^{1/p} \\
	&= \sqrt{\frac{p}{e}} (1 + o(1)) \text{ as } p \to \infty.
\end{align*}

