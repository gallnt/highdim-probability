\section{Concentration Without Independence}
This chapter mainly explores other approaches to concentration that do not rely on independence.



% ----------5.1----------
\subsection{Cencentration of Lipschitz Functions on the Sphere}
For a random vector $X$ in $\mathbb{R}^n$ and a function $f: \mathbb{R}^n \to \mathbb{R}$. When does the 
random variable $f(X)$ concentrate, i.e.
\[ f(X) \approx \mathbb{E}[f(X)] \text{ with high probability? } \]
If $X$ is normal and $f$ is linear, this is easy: $f(X)$ is normal (\cref{cor:3.3.2}) and concentrates well 
(\cref{prop:2.1.2}).

What about for general \textit{nonlinear} functions $f$? We can't expect good concentration for any $f$, 
but if $f$ does not oscillate too wildly, we might get good concentration. Namely, we'll use Lipschitz 
functions to rule out these oscillations:


\subsubsection{Lipschitz Functions}
\begin{definition}[]
\label{def:5.1.1}
Let $(X, d_X)$ and $(Y, d_Y)$ be metric spaces. A function $f:X \to Y$ is called \underline{Lipschitz} if 
there exists $L \in \mathbb{R}$ such that 
\[ d_Y(f(u), f(v)) \leq L \cdot d_X(u, v) \text{ for every } u, v \in X. \]

The infimum of all $L$ in this definition is called the \underline{Lipschitz norm} because of $f$ and is denoted 
$\lVert f \rVert_{\mathrm{Lip}}$.

If $\lVert f \rVert_{\mathrm{Lip}} \leq 1$, $f$ is called a \underline{contraction}.
\end{definition}

(\textbf{Important}) Technically the Lipschits norm is only a seminorm, since it vanishes on nonzero constant 
functions. It's called a norm in the book for brevity.

The class of Lipschitz functions sits between differentiable and uniformly continuous: 
\[ f \text{ is differentiable } \implies f \text{ is Lipschitz } \implies f \text{ if uniformly continuous.} \]

Moreover, from Exercise 5.1,
\[ \lVert F \rVert_{\mathrm{Lip}} \leq \sup_{x \in \mathbb{R}^n} \lVert \nabla f(x) \rVert_{2}. \]

\begin{example}[]
\label{ex:5.1.2}
Vectors, matries, and norms define natural Lipschitz functions:
\begin{enumerate}
	\item For a fixed vector $\theta \in \mathbb{R}^n$, the linear functional 
	\[ f(x) = \left\langle x, \theta \right\rangle \text{ has Lipschitz norm } 
	\lVert f \rVert_{\mathrm{Lip}} = \lVert \theta \rVert_{2}. \]
	\item More generally, any $m \times n$ matrix $A$, the linear operator 
	\[ f(x) = Ax \text{ has Lipschitz norm } \lVert F \rVert_{\mathrm{Lip}} = \lVert A \rVert_{}. \]
	\item For any norm $\lVert \cdot \rVert_{}$ on $\mathbb{R}^n$, the function 
	\[ f(x) = \lVert x \rVert_{} \]
	has Lipschitz norm equal to the smallest $L$ such that 
	\[ \lVert x \rVert_{} \leq L \lVert x \rVert_{2} \text{ for all } x \in \mathbb{R}^n. \]
\end{enumerate}
\end{example}

\begin{proof}
Exercise 5.2.
\end{proof}


\subsubsection{Concentration via Isoperimetric Inequalities}
Any Lipschitz function on the Euclidean sphere $S^{n - 1} = \{x \in \mathbb{R}^n: \ \lVert x \rVert_{2} = 1\}$ 
concentrates:

\begin{theorem}[]
\label{thm:5.1.3}
Let $X \sim \mathrm{Unif}(\sqrt{n}S^{n - 1})$. Then for any Lipschitz function $f: \sqrt{n}S^{n - 1} \to 
\mathbb{R}$ we have 
\[ \lVert f(X) - \mathbb{E}[f(X)] \rVert_{\psi_2} \leq C \lVert f \rVert_{\mathrm{Lip}}. \]
\end{theorem}

The theorem above works for the geodesic distance metric as well (Exercise 5.4).

\cref{thm:5.1.3} has been proved already for linear functions $f$. \cref{thm:3.4.5} tells us that $X$ is a 
subgaussian random vectos, and this by definition means that any lienar function of $X$ is a subgaussian 
random variable. 

To fully prove \cref{thm:5.1.3}, we need to argue that any Lipschitz function concentrates at least as well 
as a linear function. We'll use the aread of their \underline{sublevel sets} - regions of the sphere where 
$f(x) \leq a$ for a given level $a$. To do this, we'll use the \textit{isoperimetric inequality}, namely 
the one for subsets on $\mathbb{R}^n$: 

\begin{theorem}[Isoperimetric inequality on $\mathbb{R}^n$]
\label{thm:5.1.4}
Among all subsets $A \subset \mathbb{R}^n$ with given volume, the Euclidean balls have minimal area. Moreover, 
for any $\varepsilon > 0$, the Euclidean balls minimize the volume of the $\varepsilon$-neighborhood of $A$, 
defined as 
\[ A_{\varepsilon} = \{ x \in \mathbb{R}^n: \ \exists y \in A \text{ such that } \lVert x - y \rVert_{2} 
\leq \varepsilon \} = A + \varepsilon B_2^n. \] 
The figure below illustrates the isoperimetric inequality:
\begin{center}
	\includegraphics[width=0.8\textwidth]{Chapter 5/fig5-1.png}
\end{center}
\end{theorem}

A similar isoperimetric inequality holds for subsets on $S^{n - 1}$, and in this case the minimizers are the 
\underline{spherical caps} - neighborhoods of a single point. To state this principle, let $\sigma_{n - 1}$ 
denote the normalized are on the sphere $S^{n - 1}$ (The $n - 1$-dimensional Lebesgue measure).

\begin{theorem}[Isoperimetric inequality on the sphere]
\label{thm:5.1.5}
Let $\varepsilon > 0$. Then among all subsets $A \subset S^{n - 1}$ with given area $\sigma_{n - 1}(A)$, the 
spherical caps minimizer the area of the neighborhood $\sigma_{n - 1}(A_{\varepsilon})$, where 
\[ A_{\varepsilon} := \{ x \in \mathbb{R}^n: \ \exists y \in S^{n - 1} \text{ such that } 
\lVert x - y \rVert_{2} \leq \varepsilon \}. \]
\end{theorem}


\subsubsection{Blow-up of Sets on the Sphere}
The isoperimetric inequality leads to a remarkable and counterintuitive result: if a set $A$ covers at least 
half of the sphere in area, its $\varepsilon$-neighborhood $A_{\varepsilon}$ will cover most of the sphere. 
To simplify things in view of \cref{thm:5.1.3}, we'll operate on the sphere with radius $\sqrt{n}$.

\begin{lemma}[Blow-up]
\label{lem:5.1.6}
Let $A \subset \sqrt{n}S^{n - 1}$, and let $\sigma$ denote the normalized are on that sphere. If 
$\sigma(A) \geq 1/2$, then for every $t \geq 0$, 
\[ \sigma(A_t) \geq 1 - 2 \exp{(-ct^2)}. \]
\end{lemma}

\begin{proof}
Consider the hemisphere defined by the first coordinate:
\[ H := \{x \in \sqrt{n}S^{n - 1}: \ x_1 \leq 0 \}. \]
By assumption, $\sigma(A) \geq 1/2 = \sigma(H)$, hence the isoperimetric inequality (\cref{thm:5.1.5}) implies 
that 
\[ \sigma(A_t) \geq \sigma(H_t). \]
The neighborhood $H_t$ of the hemisphere $H$ is a spherical cap (a portion of a sphere cut off by a plane), 
and we could compute its area directly, but 
it is easier to use \cref{thm:3.4.5} instead, which states that a random vector $X \sim \mathrm{Unif}
(\sqrt{n}S^{n - 1})$ is subgaussian, and $\lVert X \rVert_{\psi_2} \leq C$. Since $\sigma$ is the uniform 
probability measure on the sphere, it follows that 
\[ \sigma(H_t) = P(X \in H_t). \]
Now, the definition of the neighborhood implies that 
\[ \{ x \in \sqrt{n}S^{n - 1}: \ x_1 \leq t / \sqrt{2} \} \subset H_t. \]
Thus
\[ \sigma(H_t) \geq P(X_1 \leq t / \sqrt{2}) \geq 1 - 2 \exp{(-ct^2)}. \]
The last inequality holds because $\lVert X_1 \rVert_{\psi_2} \leq \lVert X \rVert_{\psi_2} \leq C$. Then the 
lemma is proved because $\sigma(A_t) \geq \sigma(H_t)$.
\end{proof}

\begin{remark}[A more dramatic blow-up]
\label{rmk:5.1.7}
The $1/2$ value for the area in \cref{lem:5.1.6} was arbitrary, and can be replaced with any constant, or even 
an exponentially small quantity (Exercise 5.3)!
\end{remark}

\begin{remark}[A zero-one law]
\label{rmk:5.1.8}
The blow-up phenomenen we just saw can be quite counterintuitive at first. However, this is a typical p
phenomenon in high dimensions. It is similar to \textit{zero-one laws} in probability theory, which basically 
say that events influenced by many random variables tend to have probabilities zero or one.
\end{remark}


\subsubsection{Proof of Theorem 5.1.3}
WLOG, we can assume that $\lVert f \rVert_{\mathrm{Lip}} = 1$. Let $M$ denote the median of $f(X)$, which 
by definition satisfies 
\[ P(f(X) \leq M) \geq \frac{1}{2} \text{ and } P(f(X) \geq M) \geq \frac{1}{2}. \]
Consider the sublevel set 
\[ A := \{x \in \sqrt{n}S^{n - 1}: \ f(x) \leq M \}. \]
Since $P(X \in A) \geq \frac{1}{2}$, \cref{lem:5.1.6} implies that 
\[ P(X \in A_t) \geq 1 - 2 \exp{(-ct^2)}. \]
On the other hand, we claim that 
\[ P(X \in A_t) \leq P(f(X) \leq M + t). \]
Indeed, if $X \in A_t$ then $\lVert X - y \rVert_{2} \leq t$ for some point $y \in A$. By definition, 
$f(y) \leq M$. Since $f$ is Lipschitz with $\lVert f \rVert_{\mathrm{Lip}} = 1$, it follows that 
\[ f(X) \leq f(y) + \lVert X - y \rVert_{2} \leq M + t. \]
Combining the two bounds above, we conclude that 
\[ P(f(X) \leq M + t) \geq 1 - 2 \exp{(-ct^2)}. \]
Repeating the argument for $-f$, we obtain a similar bound for the probability that $f(x) \geq M - t$ (do). 
Combining the two, we get a similar bound for the probability that $|f(X) - M| \leq t$, and conclude that 
\[ \lVert f(X) - M \rVert_{\psi_2} \leq C. \]
Then we can replace the median by the mean, which follows by centering (\cref{lem:2.7.8}). Therefore the proof 
is complete. $\square$



% ----------5.2----------
\subsection{Concentration on Other Metric Measure Spaces}
We can extend concentration from the sphere to other spaces as well. The proof of \cref{thm:5.1.3} relied 
on two ingredients: 
\begin{enumerate}
	\item an isoperimetric inequality,
	\item a blow-up of its minimizers.
\end{enumerate}
There are not unique to the sphere - many spaces satusfy them hence we can derive similar concentration results.

\begin{remark}[]
\label{rmk:5.2.1}
Concentration keeps the mean, median, and $L^p$ norms close. Therefore, we can always replace the mean 
$\mathbb{E}[f(X)]$ with the median (Exercise 5.6), or, if the mean is nonnegative, with the $L^p$ norm for any 
$p \geq 1$, though the constant may depend on $p$ (Exercise 5.10).
\end{remark}


\subsubsection{Gaussian Concentration}
The \underline{Gaussian measure} of a Borel set $A \subset \mathbb{R}^n$ is defined as 
\[ \gamma_n(A) := P(X \in A) = \frac{1}{(2 \pi)^{n / 2}} \int_{A}^{} e^{-\lVert x \rVert_{2}^2 / 2} \ dx \]
where $X \sim N(0, I_n)$ is the standard normal random vector in $\mathbb{R}^n$.

\begin{theorem}[Gaussian isoperimetric inequality]
\label{thm:5.2.2}
Let $\varepsilon > 0$. Then among all sets $A \subset \mathbb{R}^n$ with given gaussian measure $\gamma_n(A)$, 
the half-spaces minimize the Gaussian measure of the neighborhood $\gamma_n(A_{\varepsilon})$.
\end{theorem}

\begin{theorem}[Gaussian concentration]
\label{thm:5.2.3}
Consider a random vector $X \sim N(0, I_n)$ and a Lipschitz function $f: \mathbb{R}^n \to \mathbb{R}$ (with 
respect to the Euclidean metric). Then 
\[ \lVert f(X) - \mathbb{E}[f(X)] \rVert_{\psi_2} \leq C \lVert f \rVert_{\mathrm{Lip}}. \]
\end{theorem}

\begin{example}[]
\label{ex:5.2.4}
Two special cases of \cref{thm:5.2.3} should already be familiar: 
\begin{enumerate}
	\item For linear functions $f$, it follows since $X \sim N(0, I_n)$ is subgaussian.
	\item For the Euclidean norm $f(x) = \lVert x \rVert_{2}$, it follows from norm concentration 
	(\cref{thm:3.1.1}).
\end{enumerate}
\end{example}


\subsubsection{Hamming Cube}
The method based on isoperimetry also works on the Hamming cube $(\{0, 1\}^n, d, \mathbb{P})$ 
(\cref{def:4.2.14}), where $d(x, y)$ is the normalized Hamming distance:
\[ d(x, y) = \frac{1}{n}|\{i: x_i \neq y_i\}|. \]
The measure $\mathbb{P}$ is the uniform probability measure on the cube: 
\[ \mathbb{P}(A) = \frac{|A|}{2^n} \text{ for any } A \subset \{0, 1\}^n. \]

\begin{theorem}[Concentration on the Hamming cube]
\label{ex:5.2.5}
Consider a random vector $X \sim \{0, 1\}^n$. Then for any function $f: \{0, 1\}^n \to \mathbb{R}$ we have 
\[ \lVert f(X) - \mathbb{E}[f(X)] \rVert_{\psi_2} \leq \frac{C \lVert f \rVert_{\mathrm{Lip}}}{\sqrt{n}}. \]
\end{theorem}


\subsubsection{Symmetric Group}
A similar result holds for the symmetric group $S_n$, a set of all $n!$ permutations of $\{1, \dots, n\}$. 
We can view the symmetric group as a metric measure space $(S_n, d, \mathbb{P})$, where $d(\pi, \rho)$ is the 
normalized Hamming distance - the fraction of the symbols on which permutations $\pi$ and $\rho$ differ:
\[ d(\pi, \rho) = \frac{1}{n} |\{i: \pi(i) \neq \rho(i)\}|. \]
The measure $\mathbb{P}$ is the uniform probability measure on $S_n$:
\[ \mathbb{P}(A) = \frac{|A|}{n!} \text{ for any } A \subset S_n. \]

\begin{theorem}[Concentration on the symmetric group]
\label{thm:5.2.6}
Consider a random permutation $X \sim \mathrm{Unif}(S_n)$ and a function $f: S_n \to \mathbb{R}$. Then 
\[ \lVert f(X) - \mathbb{E}[f(X)] \rVert_{\psi_2} \leq \frac{C \lVert f \rVert_{\mathrm{Lip}}}{\sqrt{n}}. \]
\end{theorem}


\subsubsection{Riemannian Manifolds with Strictly Positive Curvature}
(Feel free to skip this if not familiar with differential geometry)

A compact connected Riemannian manifold $(M, g)$ comes with the geodesic distance $d(x, y)$, which is the 
shortest length of a curve connecting the points. Then we can define a metric measure space $(M, d, \mathbb{P})$ 
where $\mathbb{P}$ is the uniform probability measure derived by normalizing the Riemannian volume. 

Let $c(M)$ denote the infimum of the Ricci curvature tensor over all tangent vectors. Assuming $c(M) > 0$, then 
it can be proved that 
\[ \lVert f(X) - \mathbb{E}[f(X)] \rVert_{\psi_2} \leq \frac{C \lVert f \rVert_{\mathrm{Lip}}}{\sqrt{c(M)}} \]
for any Lipschitz function $f: M \to \mathbb{R}$.

To give an example, $c(S^{n - 1}) = n - 1$. Then the above gives another approach for the concentration 
inequality of the sphere.


\subsubsection{Special Orthogonal Group}
The special orthogonal group $\mathrm{SO}(n)$ consists of all $n \times n$ orthogonal matrices with determinant 
1. We can treat it as a metric measure space $(\mathrm{SO}(n), \lVert \cdot \rVert_{F}, \mathbb{P})$, with 
distance given by the Frobenius norm $\lVert A - B \rVert_{F}$ and $\mathbb{P}$ as the uniform probability 
measure.

\begin{theorem}[Concentration on the special orthogonal group]
\label{thm:5.2.7}
Consider a random orthogonal matrix $X \sim \mathrm{Unif}(\mathrm{SO}(n))$ and a function $f: \mathrm{SO}(n) 
\to \mathbb{R}$. Then 
\[ \lVert f(X) - \mathbb{E}[f(X)] \rVert_{\psi_2} \leq \frac{C \lVert f \rVert_{\mathrm{Lip}}}{\sqrt{n}}. \]
\end{theorem}

The result above can be deduced from the concentration on general Riemannian manifolds.

\begin{remark}[Haar measure]
\label{rmk:5.2.8}
To generate a random orthogonal matrix $X \sim \mathrm{Unif}(\mathrm{SO}(n))$, one way is to start with an 
$n \times n$ Gaussian random matrix $G$ with $N(0, 1)$ independent entries, and compute its SVD 
$G = U \Omega V^T$. Then the matrix of left singular vectors is uniformly distributed in $\mathrm{SO}(n)$.

The uniform probability distribution on $\mathrm{SO}(n)$ is given by 
\[ \mu(A) := P(X \in A) \text{ for } A \subset \mathrm{SO}(n). \]
This is the unique rotation-invariant probaility measure on $\mathrm{SO}(n)$, called the 
\underline{Haar measure}.
\end{remark}


\subsubsection{Grassmannian}
The Grassmannian manifold $G_{n, m}$ consists of all $m$-dimensional subspaces of $\mathbb{R}^n$. When $m = 1$, 
it can be identified with the sphere $S^{n - 1}$. Therefore the concentration on the Grassmannian includes the 
concentration on the sphere. 

We can treat $G_{n, m}$ as a metric space $(G_{n, m}, d, \mathbb{P})$, where the distance between subspaces is 
given by the operator norm
\[ d(E, F) = \lVert P_E - P_F \rVert_{} \]
where $P_E$ and $P_F$ are the orthogonal projections onto the subspaces. The probability measure is the Haar 
measure (\cref{rmk:5.2.8}). A random subspace $E$ can hence be computed by computing the image of the random 
$n \times m$ Gaussian random matric with i.i.d. $N(0, 1)$ entries.

\begin{theorem}[Concentration on the Grassmannian]
\label{thm:5.2.9}
Consider a random subspace $X \sim \mathrm{Unif}(G_{n, m})$ and a function $f: G_{n, m} \to \mathbb{R}$. Then 
\[ \lVert f(X) - \mathbb{E}[f(X)] \rVert_{\psi_2} \leq \frac{C \lVert f \rVert_{\mathrm{Lip}}}{\sqrt{n}}. \]
\end{theorem}

\begin{proof}
The proof is a bit involved: Express the Grassmannian as the quotient via the special orthogonal group:
\[ G_{n, m} = \mathrm{SO}(n) / (\mathrm{SO}(m) \times \mathrm{SO}(n - m)) \]
and use the fact that concentration carries over to quotients.
\end{proof}


\subsubsection{Continuous Cube and Euclidean Ball}
\begin{theorem}[Concentration on the continuous cube and ball]
\label{thm:5.2.10}
Let $T$ be either the cube $[0, 1]^n$ or the ball $\sqrt{n}B_2^n$. Consider a random vector $X \sim 
\mathrm{Unif}(T)$ and a Lipschitz function $f; T \to \mathbb{R}$, where the Lipschitz norm is with respect 
to the Euclidean distance. Then 
\[ \lVert f(X) - \mathbb{E}[f(X)] \rVert_{\psi_2} \leq C \lVert f \rVert_{\mathrm{Lip}}. \]
\end{theorem}

\begin{proof}
Exercises 5.12 \& 5.13.
\end{proof}


\subsubsection{Densities of the Form \texorpdfstring{$e^{-U(x)}$}{}}
The push forward method from the previous section can be applied to many other distributions in $\mathbb{R}^n$. 
For example, suppose a random vector $X$ has a density of the form
\[ f(x) = e^{-U(x)} \]
for some function $U: \mathbb{R}^n \to \mathbb{R}$. For example, $X \sim N(0, I_n)$, the normal density gives 
$U(x) = \lVert x \rVert_{2}^2 + c$ where $c$ is constant (dependent on $n$ but not on $x$), and Gaussian 
concentration holds for $X$.

In general, we would expect that if $U$ has curvature at least like $\lVert x \rVert_{2}^2$, then there would be 
at least Gaussian concentration. As the theorem below shows, this depends on the Hessian of $U$:

\begin{theorem}[]
\label{thm:5.2.11}
Consider a random vector $X$ in $\mathbb{R}^n$ whose density has the form $e^{-U(x)}$ for some function 
$U: \mathbb{R}^n \to \mathbb{R}$. Assume there exists $\kappa > 0$ such that 
\[ \nabla^2 U(x) \succcurlyeq \kappa I_n \text{ for all } x \in \mathbb{R}^n. \]
Then any Lipschitz function $f: \mathbb{R}^n \to \mathbb{R}$ satisfies
\[ \lVert f(X) - \mathbb{E}[f(X)] \rVert_{\psi_2} \leq \frac{C \lVert f \rVert_{\mathrm{Lip}}}{\sqrt{\kappa}}. \]
\end{theorem}

\begin{proof}
THe proof uses semigroup methods, which are not covered in the text.
\end{proof}


\subsubsection{Random Vectors with Independent Bounded Coordinates}
There is a remarkable partial generalization of \cref{thm:5.2.10} for random vectors $X$ with independent 
coordinates that have arbitrary bounded distributions (not just uniform). By scaling, we can assume WLOG that 
$|X_i| \leq 1$.

\begin{theorem}[Talagrand concentration inequality]
\label{thm:5.2.12}
Consider a random vector in $\mathbb{R}^n$, 
$X = (X_1, \dots, X_n)$ whose coordinates are independent and satisfy $|X_i| \leq 1$ 
almost surely. Then for any Lipschitz function $f: [-1, 1]^n \to \mathbb{R}$, 
\[ \lVert f(X) - \mathbb{E}[f(X)] \rVert_{\psi_2} \leq C \lVert f \rVert_{\mathrm{Lip}}. \]
\end{theorem}



% ----------5.3----------
\subsection{Application: Johnson-Lindenstrauss Lemma}
Suppose we have $N$ data points in $\mathbb{R}^n$ where the dimension $n$ is very large. Can we reduce the 
dimension without losing the geometry of the data? The simplest way is to project onto a low-dimensional 
subspace 
\[ E \subset \mathbb{R}^n, \ \dim{(E)} := m \ll n, \]
see Figure 5.2 below. How shall we choose the subspace $E$, and how small should its dimension $m$ be?

\begin{center}
	\includegraphics[width=0.8\textwidth]{Chapter 5/fig5-2.png}
\end{center}

The Johnson-Lindenstrauss Lemma states that the geometry of data is well preserved if we choose $E$ to be a 
\textit{random subspace} of dimension 
\[ m \asymp \log_{}{N}. \]
Here we say that $E$ is a random $m$-dimensional subspace in $\mathbb{R}^n$ uniformly distributed in $G_{n, m}$, 
i.e. 
\[ E \sim \mathrm{Unif}(G_{n,m}), \]
if $E$ is a random $m$-dimensional subspace of $\mathbb{R}^n$ whose distribution is rotation invariant, i.e. 
\[ P(E \in \mathcal{E}) = P(U(E) \in \mathcal{E}) \]
for any fixed subset $\mathcal{E} \in G_{n,m}$ and $n \times n$ orthogonal matrix $U$.

\begin{theorem}[Johnson-Lindenstrauss Lemma]
\label{thm:5.3.1}
Let $\mathcal{X}$ be a set of $N$ points in $\mathbb{R}^n$ and $\varepsilon > 0$. Assume that 
\[ m \geq C \varepsilon^{-2}\log_{}{N}. \]
Let $P$ be the orthogonal projection in $\mathbb{R}^n$ onto a random $m$-dimensional subspace $E \sim 
\mathrm{Unif}(G_{n, m})$. Then, with probability at least $1 - 2 \exp{(-c \varepsilon^2 m)}$, the scaled 
projection $Q = \sqrt{n/m}P$ is an approximate isometry on $\mathcal{X}$:
\[ (1 - \varepsilon)\lVert x - y \rVert_{2} \leq \lVert Qx - Qy \rVert_{2} 
\leq (1 + \varepsilon)\lVert x - y \rVert_{2} \texxt{ for all } x, y \in \mathcal{X}. \]
\end{theorem}
The proof will be based on concentration of Lipschitz functions on the sphere in Section 5.1 We 
use it to examine how the random projection $P$ acts on the fixed vector $x - y$, then take the union bound over 
all $N^2$ differences $x - y$.

\begin{lemma}[Random Projection]
\label{lem:5.3.2}
Let $P$ be a projection in $\mathbb{R}^n$ onto a random $m$-dimensional subspace $E \sim 
\mathrm{Unif}(G_{n,m})$. Fix any $z \in \mathbb{R}^n$ and $\varepsilon > 0$. Then,
\begin{enumerate}
	\item $(\mathbb{E}\left[ \lVert Pz \rVert_{2}^2 \right])^{1/2} = \sqrt{\frac{m}{n}} \lVert z \rVert_{2}$.
	\item With probability at least $1 - 2 \exp{(-c \varepsilon^2 m)}$, we have 
	\[ (1 - \varepsilon)\sqrt{\frac{m}{n}}\lVert z \rVert_{2} \leq \lVert Pz \rVert_{2} 
	\leq (1 + \varepsilon)\sqrt{\frac{m}{n}}\lVert z \rVert_{2}. \]
\end{enumerate}
\end{lemma}

\begin{proof}
Without loss of generality, we may assume that $\lVert z \rVert_{2} = 1$. Now switch the view. A random 
$m$-dimensional subspace $E$ can be obtained by randomly rotating some fixed subspace, such as the coordinate 
subspace $\mathbb{R}^m$. But instead of fixing $z$ and randomly rotate $\mathbb{R}^m$, we cna fix the the 
subspace $E = \mathbb{R}^m$ and randomly rotate $z$, which makes $z$ uniformly distributed on the sphere:
\[ z \sim \mathrm{Unif}(S^{n - 1}). \]
By rotation invariance, $Pz$ has the same distribution!

(a) Since $P$ is the projection onto the first $m$ coordinates in $\mathbb{R}^n$, 
\[ \mathbb{E}\left[ \lVert Pz \rVert_{2}^2 \right] = \mathbb{E}\left[ \sum_{i = 1}^{m} z_i^2 \right] 
= \sum_{i = 1}^{m}\mathbb{E}\left[ z_i^2 \right] = m \mathbb{E}\left[ z_1^2 \right], \]
because the coordinates $z_i$ of the random vector $z \sim \mathrm{Unif}(S^{n - 1})$ are identically 
distributed. To compute $\mathbb{E}\left[ z_1^2 \right]$, note that $\sum_{i = 1}^{n}z_i^2 = 1$. Taking 
expectations on both sides, we obtain 
\[ \sum_{i = 1}^{n}\mathbb{E}\left[ z_i^2 \right] = 1 \implies \mathbb{E}\left[ z_1^2 \right] = \frac{1}{n}. \]
Then, putting this into the equation above, we have 
\[ \mathbb{E}\left[ \lVert Pz \rVert_{2}^2 \right] = \frac{m}{n}. \]

(b) $x \mapsto \lVert Px \rVert_{2}$ is a Lipschitz function on $S^{n - 1}$ with Lipschitz norm bounded by 1. 
Then from Exercise 5.5, the concentration inequality gives 
\[ P \left( \left| \lVert Px \rVert_{2} - \sqrt{\frac{m}{n}} \right| \geq t \right) 
\leq 2 \exp{(-cnt^2)}. \]
(We replaced $\mathbb{E}\left[ \lVert x \rVert_{2} \right]$ by $(\mathbb{E}\left[ \lVert x \rVert_{2}^2 \right]^
{1/2})$ in the concentration inequality using \cref{rmk:5.2.1}). Choosing $t := \varepsilon \sqrt{m/n}$, we 
complete the proof.
\end{proof}

\begin{proof}[Proof of Johnson-Lindenstrauss Lemma]
Consider the difference set 
\[ \mathcal{X} - \mathcal{X} := \{ x - y: \ x, y \in \mathcal{X} \}. \]
We would like to show that, with required probability, the inequality
\[ (1 - \varepsilon)\lVert z \rVert_{2} \leq \lVert Qz \rVert_{2} \leq (1 + \varepsilon)\lVert z \rVert_{2} \]
holds for all $z \in \mathcal{X} - \mathcal{X}$. Since $Q = \sqrt{n/m}P$, this inequality is equivalent to 
\[ (1 - \varepsilon)\sqrt{\frac{m}{n}}\lVert z \rVert_{2} \leq \lVert Pz \rVert_{2} 
\leq (1 + \varepsilon)\sqrt{\frac{m}{n}}\lVert z \rVert_{2}. \]
For any fixed $z$, \cref{lem:5.3.2} states that the above holds with probability at least $1 - 2 \exp{(-c 
\varepsilon^2 m)}$. It remains to take a union bound over $z \in \mathcal{X} - \mathcal{X}$. It follows that 
the bound above holds simultaneously for all $z \in \mathcal{X} - \mathcal{X}$, with probability at least 
\[ 1 - |\mathcal{X} - \mathcal{X}| \cdot 2 \exp{(-c \varepsilon^2 m)} \geq 1 - N^2 \cdot 
2 \exp{(-c \varepsilon^2 m)}. \]
If $m \geq C \varepsilon^{-2} \log_{}{N}$ then this probability is at least $1 - 2 
\exp{(-c \varepsilon^2 m/2)}$, as claimed. Hence the proof is done.
\end{proof}

\begin{remark}[Non-adaptive, dimension-free]
\label{rmk:5.3.3}
A remarkable feature of the JL lemma is that the dimension reduction map $A$ is \textit{non-adaptive}, 
meaning it does not depend on the data. Note also that the ambient dimension $n$ of the data plays no role. With 
more toold, we will develop more advanced versions of the JH lemma (Exercise 9,37-9.39).
\end{remark}

\begin{remark}[Optimality]
The JL lemma makes such a striking dimension reduction from $N$ to $n = O(\log_{}{N})$. Can we go even smaller, 
say $n = o(\log_{}{N})$? Exercise 5.15 shows that we can't - the log dimension is the best we can do, even with 
nonlinear maps.
\end{remark}



% ----------5.4----------
\subsection{Matrix Bernstein Inequality}
We extend generalized concentration inequalities from sums of independent random variables to sums of 
independent random matrices. We'll make a matrix version of Bernstein inequality (\cref{thm:2.9.5}) by 
replacing random variables by random matrices, and absolute value by the operator norm. No need for 
independence of entries, rows, or columns within each random matrix!

\begin{theorem}[Matrix Bernstein inequality]
\label{thm:5.4.1}
Let $X_1, \dots, X_N$ be independent, mean zero, $n \times n$ symmetric random matrices, such that 
$\lVert X_i \rVert_{} \leq K$ almost surely for all $i$. Then for every $t \geq 0$, 
\[ P \left( \lVert \sum_{i = 1}^{N}X_i \rVert_{} \geq t \right) \leq 
2n \exp{\left( -\frac{t^2 / 2}{\sigma^2 + Kt / 3} \right)} \]
where $\sigma^2 = \lVert \sum_{i = 1}^{N} \mathbb{E}[X_i^2] \rVert_{}$ is the operator norm of the matrix 
variance of the sum.
\end{theorem}
We can rewrite the RHS of the inequality as the mixture of subgaussian and subexponential tail, like in the 
scalar Bernstein inequality:
\[ P \left( \lVert \sum_{i = 1}^{N}X_i \rVert_{} \geq t \right) \leq 
2n \exp{\left[ -c \cdot \min_{}\left( \frac{t^2}{\sigma^2}, \frac{t}{K} \right) \right]}. \]
The proof is similar to that of the scalar version: Repeat the MGF argument, swapping scalars with matrices. 
However, there is a big problem: Matrix multiplication is not commutative! Therefore we need some matrix 
calculus knowledge first.


\subsubsection{Matrix Calculus}
For an $n \times n$ symmetric matrix $X$, operations such as inversion or squaring only affect eigenvalues. 
For example, if the spectral decomposition of $X$ is $X = \sum_{i = 1}^{n} \lambda_i u_iu_i^T$, then
\[ X^{-1} = \sum_{i = 1}^{n} \frac{1}{\lambda_i}u_iu_i^T, \ 
X^2 = \sum_{i = 1}^{n} \lambda_i^2 u_iu_i^T, \ 
2I_n - 5X^3 = \sum_{i = 1}^{n} (2 - 5 \lambda_i^3)u_iu_i^T. \]
This suggest that for symmetric matrices, applying arbitrary functions on the matrices is equivalent to 
applying them to the eigenvalues:

\begin{definition}[Functions of matrices]
\label{def:5.4.2}
For a function $f: \mathbb{R} \to \mathbb{R}$ and an $n \times n$ symmetric matrix $X$ with spectral 
decomposition as above, define 
\[ f(X) := \sum_{i = 1}^{n} f(\lambda_i)u_iu_i^T. \]
\end{definition}
This definition agrees with matrix addition and multiplication, and with Taylor series (Exercise 5.16).

Of course, matrices can be compared with each other via a \underline{partial ordering}:
\begin{definition}[Loewner order]
\label{def:5.4.3}
We write $X \succcurlyeq 0$ if $X$ is a symmetric positive semidefinite matrix. We write $X \succeq Y$ and 
$Y \preceq X$ if $X - y \succeq 0$.
\end{definition}
This is a partial ordering because there are matrices for which neither $X \succeq Y$ nor $Y \succeq X$ holds.

\begin{proposition}[Simple properties of Loewner order]
\label{prop:5.4.4}
We have
\begin{enumerate}
	\item (Eigenvalue monotonicity) $X \preceq Y$ implies $\lambda_i(X) \leq \lambda_i(Y)$ for all $i$.
	\item (Trace monotonicity) For a (weakly) increasing function $f: \mathbb{R} \to \mathbb{R}$, 
	\[ X \preceq Y \implies \mathrm{tr}(f(X)) \leq \mathrm{tr}(f(Y)). \]
	\item (Operator norm) For any $a \geq 0$, 
	\[ \lVert X \rVert_{} \leq a \iff -aI_n \preceq X \preceq aI_n. \]
	\item (Upgrading scalar to matrix inequalities) For functions $f, g: \mathbb{R} \to \mathbb{R}$, 
	\[ f(x) \leq g(x) \forall x \text{ with } |x|\leq a \implies 
	f(X) \preceq g(X) \forall X \text{ with } \lVert X \rVert_{} \leq a. \]
\end{enumerate}
\end{proposition}

\begin{proof}
(a) If $X \preceq Y$, then $Y - X \succeq 0$ hence all eigenvalues of $Y - X$ are greater than equal to 0, 
and the result follows.

(b) The eigenvalues of $f(X)$ are $f(\lambda_i(X))$. The same can be said for $f(Y)$. By part (a) and the 
assumption, $f(\lambda_i(X)) \leq f(\lambda_i(Y))$. Summing these gives the result since the trace is the sum 
of the eigenvalues.

(c) From \cref{rmk:4.1.12}, $\lVert X \rVert_{} \leq a$ implies $u^T Xu \leq a$ for all unit vectors $u$. 
Therefore $u^T(aI_n - X)u \geq 0$ for all $u$, meaning $aI_n - X \succeq 0$, thus $X \preceq aI_n$. For the 
other inequality, again from \cref{rmk:4.1.12}, $u^T Xu \geq -a$ for all unit vectors $u$. Following the 
exact procedure above gives $X \succeq -aI_n$.

(d) By considering $g - f$, we can assume that $f = 0$. If $\lVert X \rVert_{} \leq a$, then all eigenvalues 
of $X$ satisfy $|\lambda_i| \leq a$, which implies $g(\lambda_i) \geq 0$ by assumption. So, by definition, 
$g(X)$ has nonnegative eigenvalues $g(\lambda_i)$ and so $g(X) \succeq 0$.
\end{proof}

\begin{remark}[Operator norm as matric absolute value]
\label{rmk:5.4.5}
(c) of \cref{prop:5.4.4} looks quite familiar... It is a matrix version of the basic fact about absolute 
values: for $x \in \mathbb{R}$, 
\[ |x| \leq a \iff -a \leq x \leq a. \]
This makes the operator norm $\lVert \cdot \rVert_{}$ a natural matrix version of the absolute value $|\cdot|$, 
and that's why it appeares in the matrix Bernstein inequality (\cref{thm:5.4.1}).
\end{remark}

\begin{remark}[Matrix monotonicity]
\label{rmk:5.4.6}
Can we strenghten trace monotonicity (\cref{prop:5.4.4} (b)) to matrix monotonicity, i.e.
\[ X \preceq Y \implies f(X) \preceq f(Y) \text{ for any weakly increasing } f: \mathbb{R} \to \mathbb{R}? \]
If $X$ and $Y$ commute, yes - but in general, no (Exercise 5.17). 

However, some functions, like $1/x$ and $\log_{}{x}$ on $[0, \infty)$, are \underline{matrix monotone}, meaning 
that the above holds even for non-commuting matrices:
\[ 0 \preceq X \preceq Y \implies X^{-1} \succeq Y^{-1} \succeq 0 \text{ and } \log_{}{X} \preceq \log_{}{Y} \]
whenever $X$ is invertible (Exercise 5.18).
\end{remark}


\subsubsection{Trace Inequalities}
Here is another identity that works for real numbers but not for matrices in general: $e^{x + y} = e^x e^y$ for 
scalars, but in Exercise 5.19, there are $n \times n$ symmetric matrices $X, Y$ such that 
\[ e^{X + Y} \neq e^X e^Y. \]
This is unfortunate, because when using the exponential moment method, we relied on this property to split 
the MGF via independence. 

Nevertheless, there are useful substitutes for the missing identity. In particular, this subsection covers two 
of them, both belonging to the rich family of \textit{trace inequalities}.

\begin{theorem}[Golden-Thompson inequality]
\label{thm:5.4.7}
For any $n \times n$ symmetric matrcies $A$ and $B$, 
\[ \mathrm{tr}(e^{A + B}) \leq \mathrm{tr}(e^A e^B). \]
\end{theorem}
Note that this does not hold for three or more matrices (we can find counterexamples)!

\begin{theorem}[Lieb inequality]
\label{thm:5.4.8}
Let $H$ be an $n \times n$ symmetric matrix. Define the function on matrices 
\[ f(X) := \mathrm{tr}(\exp{(H + \log_{}{X})}). \]
Then $f$ is concave on the space on PSD $n \times n$ symmetric matrices.
\end{theorem}

If $X$ is a random matrix, then Lieb and Jensen inequalities imply that 
\[ \mathbb{E}[f(X)] \leq f(\mathbb{E}[X]). \]
Applying this with $X = e^Z$, we obtain the following:

\begin{lemma}[Lieb inequality for random matrices]
\label{lem:5.4.9}
Let $H$ be a fixed $n \times n$ symmetric matrix and $Z$ be a random $n \times n$ symmetric matrix. Then 
\[ \mathbb{E}[\mathrm{tr}(\exp{(H + Z)})] \leq \mathrm{tr}(\exp{(H + \log_{}{\mathbb{E}[e^Z]})}). \]
\end{lemma}


\subsubsection{Proof of Matrix Bernstein Inequality}
(\textbf{Step 1: Reduction of MGF}) To bound the norm of the sum 
\[ S := \sum_{i = 1}^{N} X_i, \]
we need to control the largest and smallest eigenvalues of $S$. Consider the largest eigenvalue 
\[ \lambda_{\mathrm{max}}(S) := \max_{i} \lambda_i(S) \]
and note that 
\[ \lVert S \rVert_{} = \max_{i}|\lambda_i(S)| 
= \max_{}(\lambda_{\mathrm{max}}(S), \lambda_{\mathrm{max}}(-S)). \]
To bound $\lambda_{\mathrm{max}}(S)$, we'll use the exponential moment method again. Fix $\lambda > 0$. Via 
the typical procedure, 
\[ P(\lambda_{\mathrm{max}}(S) \geq t) 
= P(e^{\lambda \cdot \lambda_{\mathrm{max}}} \geq e^{\lambda t}) 
\leq e^{-\lambda t} \mathbb{E}[e^{\lambda \cdot \lambda_{\mathrm{max}}}]. \]
Then by \cref{def:5.4.2}, the eigenvalues of $e^{\lambda S}$ are $e^{\lambda \cdot \lambda_i(S)}$ so 
\[ E := \mathbb{E}[e^{\lambda \cdot \lambda_{\mathrm{max}}(S)}] 
= \mathbb{E}[\lambda_{\mathrm{max}}(e^{\lambda S})]. \]
Since the the eigenvalues of $e^{\lambda S}$ are all positive, the maximal eigenvalue is bounded by the sum of 
all eigenvalues, which is the trace. Therefore 
\[ E \leq \mathbb{E}[\mathrm{tr}(e^{\lambda S})]. \]

(\textbf{Step 2: Application of Lieb Inequality}) To use the Lieb inequality (\cref{lem:5.4.9}), we seperate 
the last term from the sum $S$:
\[ E \leq \mathbb{E}\left[ \mathrm{tr}\left( \exp{\left( \sum_{i = 1}^{N - 1} \lambda X_i 
+ \lambda X_N \right)} \right) \right]. \]
Condition on $(X_i)_{i = 1}^{N - 1}$ and apply \cref{lem:5.4.9} for the fixed matrix $H := \sum_{i = 1}^{N - 1} 
\lambda X_i$ and the random matrix $Z := \lambda X_N$. We get 
\[ E \leq \mathbb{E}[\mathrm{tr}(\exp{\left( \sum_{i = 1}^{N - 1} \lambda X_i 
+ \log_{}{\mathbb{E}[e^{\lambda X_N}]} \right)})]. \]
Then we continue the same procedure above: seperate $\lambda X_{N - 1}$ and apply \cref{lem:5.4.9}, and do the 
same thing for $N$ times to get 
\[ E \leq \mathrm{tr}\left( \exp{\left[ \sum_{i = 1}^{N} \log_{}{\mathbb{E}[e^{\lambda X_i}]} 
\right]} \right). \]

(\textbf{Step 3: MGF of the individual terms}) We'll bound the matrix-values MGF via the following lemma:
\begin{lemma}[Matrix MGF]
\label{lem:5.4.10}
Let $X$ be an $n \times n$ symmetric mean zero random matrix such that $\lVert X \rVert_{} \leq K$ almost 
surely. Then 
\[ \mathbb{E}[\exp{(\lambda X)}] \preceq \exp{(g(\lambda)\mathbb{E}[X^2]} 
\text{ where } g(\lambda) = \frac{\lambda^2 / 2}{1 - |\lambda|K / 3} \]
provided that $|\lambda| < 3 / K$.
\end{lemma}

\begin{proof}
First, we can bound the (scalar) exponential function by the first few terms via Taylor expansion:
\[ e^z \leq 1 + z + \frac{1}{1 - |z| / 3} \cdot \frac{z^2}{2}, \ |z| < 3. \]
(To get this inequality, write $e^Z = 1 + z + z^2 \sum_{p = 2}^{\infty} z^{p - 2} / p!$ and use the bound 
$p! \geq 2 \cdot 3^{p - 2}$). Next, apply this inequality to $z = \lambda x$. If $|x| \leq K$ and 
$|\lambda| < 3/K$ then we obtain 
\[ e^{\lambda x} \leq 1 + \lambda x + g(\lambda)x^2, \]
where $g(\lambda)$ is the same as how we defined in the statement.

Then we can upgrade this to a matrix inequality using \cref{prop:5.4.4} (d). If $\lVert X \rVert_{} \leq K$ and 
$|\lambda| < 3 / K$, then 
\[ e^{\lambda X} \preceq I + \lambda X + g(\lambda) X^2. \]
Taking expectations on both sides, since $\mathbb{E}[X] = 0$, 
\[ \mathbb{E}[e^{\lambda X}] \preceq I + g(\lambda) \mathbb{E}[X^2]. \]
To complete the proof of the lemma, let's use the inequality $1 + z \leq e^z$. We can transform this into a 
matrix inequality via \cref{prop:5.4.4} (d) and get $I + Z \preceq e^Z$ holds for all matrices $Z$, and in 
particular for $Z = g(\lambda)\mathbb{E}[X^2]$.
\end{proof}

(\textbf{Step 4: Completion of the proof}) Using \cref{lem:5.4.10}, we obtain 
\[ E \leq \mathrm{tr}\left( \exp{\left( \sum_{i = 1}^{N} \log_{}{\mathbb{E}[e^{\lambda X_i}]} 
\right)} \right) \leq \mathrm{tr}(\exp{(g(\lambda)Z)}), \text{ where } Z := \sum_{i = 1}^{N} \mathbb{E}[X_i^2]. \]where we used matric monotonicity of $\ln{x}$ (\cref{rmk:5.4.6}) to take logarithms on both sides, summed up 
the results, and used trace monotonicity (\cref{prop:5.4.4} (b)) to take traces of the exponential of both sides.

Since the trace of $\exp{(g(\lambda)Z)}$ is a sum of $n$ positive eigenvalues, it is bounded by $n$ times the 
maximum eigenvalue, hence 
\begin{align*}
	E 
	&\leq n \lambda_{\mathrm{max}}(\exp{(g(\lambda)Z)}) \\
	&= m \exp{(g(\lambda) \lambda_{\mathrm{max}}(Z))} \\
	&= n \exp{(g(\lambda) \lVert Z \rVert_{})} \quad (\text{Since } Z \succeq 0) \\
	&= n \exp{(g(\lambda)\sigma^2)} \quad (\text{By definition of } \sigma).
\end{align*}
Plugging in this bounde for $E = \mathbb{E}[e^{\lambda \cdot \lambda_{\mathrm{max}}(S)}]$ into the original 
equation gives
\[ P(\lambda_{\mathrm{max}}(S) \geq t) \leq n \exp{(-\lambda t + g(\lambda)\sigma^2)}. \]
The above is a bound that holds for any $\lambda > 0$ as long as $|\lambda| < 3/K$, so we can minimize it in 
$\lambda$. Better yet, instead of compuiting the exact minimum (which can be quite ugly), we can choose the 
following value: $\lambda = t / (\sigma^2 + Kt / 3)$, and substituting this value back gives 
\[ P(\lambda_{\mathrm{max}}(S) \geq t) \leq n \exp{\left( -\frac{t^2 / 2}{\sigma^2 + Kt / 3} \right)}. \]
Repeating the argument for $-S$, we will get the same bound as the above, and summing up the two bounds 
completes the proof. $\square$

\begin{remark}[Matrix Bernstein Inequality: expectation]
\label{rmk:5.4.11}
Matrix Bernstein inequality gives a high-probability bound. It can be turned into a simpler (but less 
informative) expectation bound in a standard way. Using \cref{thm:5.4.1} and the integrated tail formula 
(\cref{lem:1.6.1}), we can deduce that (Exercise 5.20)
\[ \mathbb{E}\left[ \lVert \sum_{i = 1}^{N} X_i \rVert_{} \right] 
\lesssim \lVert \sum_{i = 1}^{N} \mathbb{E}[X_i^2] \rVert_{}^{1/2} \sqrt{\log_{}{(2n)}} + K \log_{}{(2n)} \]
where the $\lesssim$ symbol hides an absolute constant factor. In the scalar case $(n = 1)$, an expectation 
bound is trivial: the variance of sum formula gives 
\[ \mathbb{E}\left[ \left| \sum_{i = 1}^{N} X_i \right| \right] 
\leq \left( \mathbb{E}\left[ \left| \sum_{i = 1}^{N} X_i \right|^2 \right] \right)^{1/2} 
= \left( \sum_{i = 1}^{N} \mathbb{E}[X_i^2] \right)^{1/2}. \]
\end{remark}

\begin{remark}[The logarithmic price]
For the equation in \cref{rmk:5.4.11}, the high-dimensional version differs the 1-dimensional one by just 
a logarithmic factor. This is a surprisingly small price for high dimensions! Moreover, this price is in 
essentially optimal - Exercise 5.28 gives an example of why we can't get rid of it.
\end{remark}


\subsubsection{Matrix Hoeffding and Khintchine Inequalities}
\begin{theorem}[Matrix Hoeffding inequality]
\label{thm:5.4.13}
Let $\varepsilon_1, \dots, \varepsilon_N$ be independent Rademacher random variables and $A_1, \dots A_N$ be 
any (fixed) symmetric $n \times n$ matrices. Then for any $t > 0$, 
\[ P \left( \lVert \sum_{i = 1}^{N} \varepsilon_i A_i \rVert_{} \geq t \right) 
\leq 2n \exp{\left( -\frac{t^2}{2 \sigma^2} \right)} \]
where $\sigma^2 = \lVert \sum_{i = 1}^{N} A_i^2 \rVert_{}$.
\end{theorem}

\begin{proof}
Exercise 5.21.
\end{proof}

\begin{theorem}[Matric Khintchine inequality]
\label{thm:5.4.14}
Let $\varepsilon_1, \dots, \varepsilon_N$ be independent Rademacher random variables and $A_1, \dots A_N$ be 
any (fixed) symmetric $n \times n$ matrices. Then for every $p \in [1, \infty)$, we have 
\[ \left( \mathbb{E}\left[ \lVert \sum_{i = 1}^{N} \varepsilon_i A_i \rVert_{}^p \right] \right)^{1/p} 
\leq C \sqrt{p + \log_{}{n}} \lVert \sum_{i = 1}^{N} A_i^2 \rVert_{}^{1/2}. \]
\end{theorem}

\begin{proof}
Exercise 5.22. Use the matrix Hoeffding inequality.
\end{proof}

\begin{remark}[Non-symmetric, rectangular matrices]
\label{rmk:5.4.15}
Matrix concentration inequalities easily extend to rectangular matrices using the \textit{Hermitian dilation} 
introduced in Exercise 4.14. Replace each matrix $X_i$ with the symmetric block matrix 
\[ \begin{bmatrix}
0 & X_i \\
X_i^T & 0 \\
\end{bmatrix} \]
and apply usual matrix concentration. We can get the matrix Bernstein (Exercise 5.23) and Khintchine (Exercise 
5.24) inequalities for rectangular matrices this way.
\end{remark}



% ----------5.5----------
\subsection{Application: Community Detection in Sparse Networks}
In section 4.5, the method of \textit{spectral clustering} was introduced, which is a basic method for 
community detection in networks. We showed that it works for relatively dense networks, where the expected 
average degree is $\gtrsim \sqrt{n}$. Now, using the matrix Bernstein inequality, we will show that spectral 
clustering actually works for much sparser networks, with an expected average degree as low as $O(\log_{}{n})$.

\begin{theorem}[Spectral clustering for sparse stochastic block model]
\label{thm:5.5.1}
Let $G \sim G(n, p, q)$ where $p = a/n, q = b/n$ and $b < a < 3b$. Assume that 
\[ (a - b)^2 \geq Ca \log_{}{n}. \]
Then, with probability at least 0.99, the spectral clustering algorithm identifies the communities of $G$ 
with 99\% accuracy, i.e. misclassifying at most $0.01n$ vertices.
\end{theorem}

\begin{proof}
We'll follow that same proof as that from Section 4.5, but with a sharper error bound.

\textbf{Step 1: Decomposition.} Again, let's split $A$ into the deterministic and random parts:
\[ A = D + R \text{ where } D = \mathbb{E}\left[ A \right]. \]
Before, the analysis is mostly on the deterministic matrix $D$, where the second largest eigenvector has $\pm 1$ 
coefficients representing community membership. Now we have to analyze the random part 
\[ R = A - \mathbb{E}\left[ A \right]. \]
Let's decompose it entry by entry, keeping symmetry in mind. We can write $R$ as a sum of independent, mean-zero 
random matrices $Z_{ij}$ that isolate entries $(i, j)$ and $(j, i)$:
\[ R = \sum_{i \leq j}^{}Z_{ij}, \text{ where } Z_{ij} = \begin{cases}
	R_{ij}(e_ie_j^T + e_je_i^T) $\text{ if } i < j, \\
	R_{ii}e_ie_i^T &\text{ if } i = j
\end{cases}. \]

\textbf{Step 2: Bounding the error}. Since $A_{ij} \in \{ 0, 1 \}$, 
\[ |R_{ij}| \leq 1 \implies \lVert Z_{ij} \rVert_{} = \lVert R_{ij} \rVert_{} \leq 1 \implies 
\lVert R_{ij}Z_{ij} \rVert_{} \leq 1. \]
Then by applying the matrix Bernstein inequality (\cref{rmk:5.4.11}) combined with Markov's inequality, we 
obtain with probability at least 0.99:
\[ \lVert R \rVert_{} \lesssim \sigma \sqrt{\log_{}{n}} + \log_{}{n} \text{ where } 
\sigma^2 = \lVert \mathbb{E}\left[ \sum_{i \leq j}^{} Z_{ij}^2 \right] \rVert_{}. \]
Let's compute $\sigma^2$. A quick check shows thaaht $Z_{ij}^2$ is a diagonal matrix:
\[ Z_{ij}^2 = \begin{cases}
	R_{ij}^2 (e_ie_i^T + e_je_j^T) &\text{ if } i < j, \\
	R_{ii}^2 e_ie_i^T &\text{ if } i = j
\end{cases}. \]
Then, by symmetry, 
\[ \sum_{i \leq j}^{}Z_{ij}^2 = \sum_{i \leq j}^{}R_{ij}^2 (e_ie_i^T + e_je_j^T) + 
\sum_{i}^{} R_{ii}^2 e_ie_i^T = \sum_{i = 1}^{n} \left( \sum_{j = 1}^{n} R_{ij}^2 \right) e_ie_i^T. \]
This is a diagonal matrix, and so is its expectation. Thus 
\[ \sigma^2 = \lVert \mathbb{E}\left[ \sum_{i \leq j}^{}Z_{ij}^2 \right] \rVert_{} = 
\max_{i = 1, \dots, n} \sum_{j = 1}^{n} \mathbb{E}\left[ R_{ij}^2 \right] \]
since the operator norm of a diagonal matrix is the maximal absolue value of its entries (Exercise 4.3 (b)). 
Recall that $R_{ij} = A_{ij} - \mathbb{E}\left[ A_{ij} \right]$. In the stochastic block model, $A_{ij}$ is 
either $\mathrm{Ber}(p)$ or $\mathrm{Ber}(q)$. So $\mathbb{E}\left[ R_{ij}^2 \right] = \mathrm{Var}(A_{ij}) 
\leq p$ since $p > q$. Thus 
\[ \sigma^2 \leq np = a, \]
and substituting this into the initial bound for $\lVert R \rVert_{}$, we get 
\[ \lVert R \rVert_{} \lesssim \sqrt{a \log_{}{n}} + \log_{}{n} \lesssim \sqrt{a \log_{}{n}} \]
because the assumption implies that $a \gtrsim \log_{}{n}$. 

\textbf{Step 3: Applying Davis-Kahan.} Let's apply \cref{thm:4.1.15} (see Exercise 4.16) to $D$ and $A$, 
focusing on the second largest eigenvalue. As we noted in Section 4.5.4, the seperation between $\lambda_2(D)$ 
of $D$ and the rest of the spectrum is
\[ \delta = \min_{}(\lambda_2(D), \lambda_1(D) - \lambda_2(D)) = \min_{}\left( \frac{p - q}{2}, q \right)n 
= \frac{a - b}{2} \]
since $a \leq 3b$ by assumption. Using the bound on $R$ from the end of Step 2, the Davis-Kahan inequality 
guarantees that for some $\theta \in \{ -1, 1 \}$, the distance between the \textit{unit} eigenvectors of $D$ 
and $A$ (denoted with bars) satisfies 
\[ \lVert \bar{u}_2(D) - \theta \bar{u}_2(A) \rVert_{2} \leq \frac{2 \lVert R \rVert_{}}{\delta} 
\leq \frac{C_1 \sqrt{a \log_{}{n}}}{a - b} < \frac{1}{10} \]
if we choose the constant $C$ in the assumption of the theorem to be large enough. Multiply both sides by 
$\sqrt{n}$ to get 
\[ \lVert u_2(D) - \theta u_2(A) \rVert_{2} \lesssim \frac{\sqrt{n}}{10}. \]
Since all coefficients of $u_2(D)$ are $\pm 1$ and correctly identify community membership, it follows that at 
least $99\%$ of the coefficients in $\theta u_2(A)_j$ have the same sign as $u_2(D)_j$, and thus correctly 
identify the community membership.
\end{proof}

\begin{remark}[Sparsity]
\label{rmk:5.5.2}
The sparsest graphs for which \cref{thm:5.5.1} is nontrivial have expected average degree
\[ \frac{n(p + q)}{2} = \frac{a + b}{2} \asymp \log_{}{n}, \]
That's way sparser than the bound of $O(\sqrt{n})$ that we have acheived previously (\cref{rmk:4.5.3})! 
\end{remark}



% ----------5.6----------
\subsection{Application: Covariance Estimation for General Distributions}
In Section 4.7, we learned how to estimate the covariance metrix of a subgaussian distribution in $\mathbb{R}^n$ 
from a sample of size $O(n)$. Now, we drop the subgaussian assumption, making this work for much broader 
distributions, even discrete ones. The trade-odd is just a logarithmic oversampling factor!

For noration, we denote the sample covariance matrix as 
\[ \Sigma_m = \frac{1}{m}\sum_{i = 1}^{m}X_i X_i^T. \]
If $X$ has zero mean, then $\Sigma$ is the covariance matrix of $X$, and $\Sigma_m$ is the sample covariance 
matrix.

\begin{theorem}[General covariance estimation]
\label{thm:5.6.1} 
Let $X$ be a random vector in $\mathbb{R}^n \ (n \geq 2)$. Assume that for some $K \geq 1$, 
\[ \lVert X \rVert_{2} \leq K (\mathbb{E}\left[ \lVert X \rVert_{2}^2 \right])^{1/2} \text{ almost surely.} \]
Then for every positive integer $m$, we have 
\[ \mathbb{E}\left[ \lVert \Sigma_m - \Sigma \rVert_{} \right] \leq C \left( 
\sqrt{\frac{K^2 n \log_{}{n}}{m}} + \frac{K^2 n \log_{}{n}}{m} \right) \lVert \Sigma \rVert_{}. \]
\end{theorem}

\begin{proof}
By \cref{prop:3.2.1} (b), we have $\mathbb{E}\left[ \lVert X \rVert_{2}^2 \right] = \mathrm{tr}(\Sigma)$, hence 
the condition in the theorem becomes 
\[ \lVert X \rVert_{2}^2 \leq K^2 \mathrm{tr}(\Sigma) \text{ almost surely. } \]
Apply the expected version of the matrix Bernstein inequality (\cref{rmk:5.4.11}) for the sum of i.i.d. mean 
zero random matrices $X_iX_i^T - \Sigma$ and get 
\[ \mathbb{E}\left[ \lVert \Sigma_m - \Sigma \rVert_{} \right] = 
\frac{1}{m} \mathbb{E}\left[ \lVert \sum_{i = 1}^{m} (X_iX_i^T - \Sigma) \rVert_{} \right] 
\lesssim \frac{1}{m} (\sigma \sqrt{\log_{}{n}} + M \log_{}{n}) \]
where 
\[ \sigma^2 = \lVert \sum_{i = 1}^{m} \mathbb{E}\left[ (X_iX_i^t - \Sigma)^2 \right] \rVert_{} 
= m \lVert \mathbb{E}\left[ (XX^T - \Sigma)^2 \right] \rVert_{} \]
and $M$ is any number chosen so that 
\[ \lVert XX^T - \Sigma \rVert_{} \leq M \text{ almost surely. } \]
To complete the proof, it remains to bound $\sigma^2$ and $M$.

Let's start with $\sigma^2$. By expanding the square, 
\[ \mathbb{E}\left[ (XX^T - \sigma)^2 \right] = \mathbb{E}\left[ (XX^T)^2 \right] - \Sigma^2 
\lesssim \mathbb{E}\left[ (XX^T)^2 \right]. \quad (*) \]
Furthermore, the assumption at the beginning of the proof gives 
\[ (XX^T)^2 = \lVert X \rVert_{}^2 XX^T \lesssim K^2 \mathrm{tr}(\Sigma) XX^T. \]
Taking expectations on both sides, we get 
\[ \mathbb{E}\left[ (XX^T)^2 \right] \lesssim K^2 \mathrm{tr}(\Sigma) \Sigma. \]
Substituting this bound into $(*)$, we get a bound for $\sigma^2$:
\[ \sigma^2 \leq K^2 m \mathrm{tr}(\Sigma) \lVert \Sigma \rVert_{}. \]


On the other hand, bounding $M$ is easier:
\begin{align*}
	\lVert XX^T - \Sigma \rVert_{} 
	&\leq \lVert X \rVert_{2}^2 + \lVert \Sigma \rVert_{} \quad \text{(By triangle inequality)} \\
	&\leq K^2 \mathrm{tr}(\Sigma) + \lVert \Sigma \rVert_{} \quad \text{(By assumption)} \\
	&\leq 2K^2 \mathrm{tr}(\Sigma) =: M. \quad (\lVert \Sigma \rVert_{} \leq \mathrm{tr}(\Sigma) \text{ and } 
	K \geq 1).
\end{align*}
Substitute the bounds for $\sigma$ and $M$ into the overall bound, we get 
\[ \mathbb{E}\left[ \lVert \Sigma_m - \Sigma \rVert_{} \right] \leq 
\frac{1}{m} \left( \sqrt{K^2m \mathrm{tr}(\Sigma) \lVert \Sigma \rVert_{}} \cdot \log_{}{n} 
+ 2K^2 \mathrm{tr}(\Sigma) \cdot \log_{}{n} \right). \]
Finally, plugging in the bound $\mathrm{tr}(\Sigma) \leq n \lVert \Sigma \rVert_{}$ completes the proof.
\end{proof}

\begin{remark}[Sample complexity]
\label{rmk:5.6.2}
\cref{thm:5.6.1} shows that for any $\varepsilon \in (0, 1)$, we can estimate the covariance matrix with a 
small relative error: 
\[ \mathbb{E}\left[ \lVert \Sigma_m - \Sigma \rVert_{} \right] \leq \varepsilon \lVert \Sigma \rVert_{}, \]
as long as the sample size is 
\[ m \asymp \varepsilon^{-2} n \log_{}{n}. \]
Compared to the sample complexity $m \asymp \varepsilon^{-2}n$ for subgaussan distributions (\cref{rmk:4.7.2}), 
dropping the subgaussian assumption costs just a small logarithmic oversampling factor! In general, this factor 
cannot be dropped (Exercise 5.28).
\end{remark}

\begin{remark}[Low-dimensional distributions]
\label{rmk:5.6.3}
At the end of proof of \cref{thm:5.6.1}, we used a rough bound $\mathrm{tr}(\Sigma) \leq n \lVert \Sigma 
\rVert_{}$. But instead, we can express the conclusion via the \textit{effective rank} of $\Sigma$:
\[ r = r(\Sigma) = \frac{\mathrm{tr}(\Sigma)}{\lVert \Sigma \rVert_{}} \]
and get a sharper bound 
\[ \mathbb{E}\left[ \lVert \Sigma_m - \Sigma \rVert_{} \right] \leq C \left( 
\sqrt{\frac{K^2 r \log_{}{n}}{m}} + \frac{K^2 r \log_{}{n}}{m} \right) \lVert \Sigma \rVert_{}. \]
It shows that a sample of size 
\[ m \asymp \varepsilon^{-2} r \log_{}{n} \]
is enough to estimate the covaraince matrix. Since $r \leq n$, this sample size is at least as small as the 
value that we had estimated above. It is even much smaller for \textit{approximately low-dimensional} 
distributions that concentrate near lower-dimensional subspaces.
\end{remark}

\begin{remark}[Effective and stable rank of a matrix]
\label{rmk:5.6.4}
What does the effective rank from \cref{rmk:5.6.3} really tell us about a PSD matrix $\Sigma$? TO get an idea, 
write it as the sum of eigenvalues divided by the largest one:
\[ r(\Sigma) = \frac{\sum_{i = 1}^{n} \lambda_i(\Sigma)}{\max_{i} \lambda_i(\Sigma)}. \]
This is always bounded by the actual rank (number of nonzero eigenvalues) and can be much smaller for 
``approximately" low-rank matrices - ones having only a few large eigenvalues. A related idea is the 
\textit{stable rank}, defined for any matrix $A$
\[ s(A) = \frac{\lVert A \rVert_{F}^2}{\lVert A \rVert_{}^2} 
= \frac{\sum_{i = 1}^{n} \sigma_i^2(A)}{\max_{i} \lambda_i(\sigma)} = r(A^T A) = r(AA^T) \]
where $\sigma_i$ denotes the singular values. Both are ``soft" versions of rank that are stable under small 
changes. For some more intuition, see Exercise 5.26.
\end{remark}

\begin{remark}[High-probability guarantees]
\label{rmk:5.6.5}
We covered expectation bounds, but our argument actually gives a more informative high-probability guarantee:
\[ \lVert \Sigma_m - \Sigma \rVert_{} \leq C \left( 
\sqrt{\frac{K^2 r (\log_{}{n} + u)}{m}} + \frac{K^2 r (\log_{}{n} + u)}{m}\right) \lVert \Sigma \rVert_{} \]
with probability at least $1 - 2e^{-u}$. Here $r = \mathrm{tr}(\Sigma) / \lVert \Sigma \rVert_{} \leq n$ is 
the effective rank (Exercise 5.26).
\end{remark}

\begin{remark}[Boundedness assumption]
\label{rmk:5.6.6}
The boundedness assumption in \cref{thm:5.6.1} might seem strong, but it cannot be dropped in general: if $X$ 
is isotropic but zero with high probability, the sample is likely to consist entirely of zeros, making 
covariance estimation impossible (Exercise 5.27). However, this assumption can still be relaxed (Exercise 6.34). 
In practice, it is usually enforced by truncation - dropping a small percentage of samples with the largest norm.
\end{remark}



% ----------5.7----------
\subsection{Extra notes}
There are lots of other concentration theorems not went over in the text. A very useful one is the McDiarmid 
inequality, which generalizes the Hoeffding inequality:

\begin{theorem}[McDiarmid inequality]
\label{thm:5.7.1}
Let $X = (X_1, \dots, X_N)$ be a random vector with independent entries. Let $f: \mathbb{R}^n \to \mathbb{R}$ 
be a measurable function. Assume that the value of $f(x)$ can change by at most $c_i > 0$ under an arbitrary 
change of a single coordinate of $x \in \mathbb{R}^n$. Then for any $t > 0$, 
\[ P(f(X) - \mathbb{E}[f(X)] \geq t) \leq \exp{\left( -\frac{2t^2}{\sum_{i = 1}^{N}c_i^2} \right)}. \]
\end{theorem}
