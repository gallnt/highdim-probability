\section{Quadratic Forms, Symmetrization, and Contraction}
This section concerns mostly with decoupling, concentration of quadratic forms, symmetrization, and contraction, 
which are a number of basic toold of high-dimensional probability.



% ----------6.1----------
\subsection{Decoupling}
We'll look at \underline{quadratic forms} of the form
\[ \sum_{i, j = 1}^{n} a_{ij} X_i X_j = X^T AX = \left\langle X, AX \right\rangle \]
where $A = (a_{ij})$ is an $n \times n$ coefficient matrix and $X = (X_1, \dots, X_n)$ is a random vector 
with independent coordinates. Such quadratic forms are known as \underline{chaos}.

We can compute the expectation of a chaos. If $X_i$ have zero means and unit variances, then 
\[ \mathbb{E}[X^T AX] = \sum_{i, j = 1}^{n} a_{ij} \mathbb{E}[X_i X_j] 
= \sum_{i = 1}^{n} a_{ii} = \mathrm{tr}(A). \]

However, establishing concentration on a chaos is harder, because the terms of the sum above are not 
independent. However, we can overcome this difficulty via \underline{decoupling}. We'll replace the quadratic 
form above with the bilinear form 
\[ \sum_{i, j = 1}^{n} a_{ij}  X_iX_j' = X^T AX' = \left\langle X, AX' \right\rangle,  \]
where $X' = (X_1', \dots, X_n')$ is an independent copy of $X$. Bilinear forms are easier to analyze than 
quadratic forms as they are linear in $X$. Therefore if we condition on $X'$, we may treat the bilinear form 
as a sum of independent random variables
\[ \sum_{i = 1}^{n} \left( \sum_{j = 1}^{n} a_{ij} X_j' \right) X_i = \sum_{i = 1}^{n} b_i X_i \]
with fixed coefficients $b_i$.

\begin{theorem}[Decoupling]
\label{thm:6.1.1}
Let $A$ be an $n \times n$ diagonal free matrix, i.e. all diagonal entries are zero. Let $X$ be a random vector 
in $\mathbb{R}^n$ with independent mean zero coordinates, and let $X'$ be an independent copy. Then for every 
convex function $F: \mathbb{R} \to \mathbb{R}$, 
\[ \mathbb{E}[F(X^T AX)] \leq \mathbb{E}[F(4X^T AX')]. \]
\end{theorem}

\begin{proof}
We'll replace the chaos by a partial chaos, which we extend back to the original chaos later via Jensen's 
inequality. The partial chaos is defined by 
\[ \sum_{(i, j) \in I \times I^c}^{} a_{ij}X_iX_j \]
where $I \subset \{1, \dots, n\}$ is a randomly chosen subset of indices.

(\textbf{Step 1: Randomly selecting a partial sum}) To specify a random subset of indices $I$, we'll use 
\underline{selectors} - independent Bernoulli random variables $\delta_1, \dots, \delta_n \sim_{iid} 
\mathrm{Ber}(1/2)$. We define the index set 
\[ I := \{ i: \delta_i = 1 \}. \]
Condition on $X$. Since by assumption $a_{ii} = 0$ and 
\[ \mathbb{E}[\delta_i(1 - \delta_j)] = \frac{1}{2} \cdot \frac{1}{2} = \frac{1}{4} \text{ for all } i \neq j \]
we may express the chaos as 
\[ X^T AX = \sum_{i \neq j}^{} a_{ij} X_iX_j 
= 4 \mathbb{E}_{\delta}\left[ \sum_{i \neq j}^{} \delta_i(1 - \delta_j) a_{ij} X_iX_j \right] 
= 4 \mathbb{E}_I \left[ \sum_{(i, j_ \in I \times I^C)}^{} a_{ij}X_iX_j \right]. \]
(In the expression above, the subscripts $\delta$ and $I$ indicate the source of randomness in the conditional 
expectations. Since $X$ is fixed, the expectations are taken over the random selection of 
$\delta = (\delta_1, \dots, \delta_n)$, or equivalently, the random index set $I$).

(\textbf{Step 2: Applying $F$}) Applying the function $F$ to both sides and take expectation over $X$. By 
Jensen inequality and Fubini theorem, we get 
\[ \mathbb{E}_X[F(X^T AX)] \leq \mathbb{E}_I \left[ \mathbb{E}_X \left[ 
F \left( 4 \sum_{(i, j) \in I \times I^c}^{} a_{ij}X_iX_j \right) \right] \right]. \]
It follows that there exists a realization of a subset $I$ such that 
\[ \mathbb{E}_X[F(X^T AX)] \leq \mathbb{E}_X \left[ 
F \left( 4 \sum_{(i, j) \in I \times I^c}^{} a_{ij}X_iX_j \right) \right]. \]
Fix such a realization $I$ until the end of the proof, and drop the subscript $X$ on the expectation for 
convenience. Since the random variables $(X_i){i \in I}$ are independent from $(X_j)_{j \in I^c}$, the 
distribution of the sum in the right side will not change if we replace $X_j$ by $X_j'$ hence
\[ \mathbb{E}_X[F(X^T AX)] \leq \mathbb{E} \left[ 
F \left( 4 \sum_{(i, j) \in I \times I^c}^{} a_{ij}X_iX_j' \right) \right]. \]

(\textbf{Step 3: Completing the partial sum}) It remains to complete the sum on the RHS to the sum over all 
pairs of indices. We want to show that 
\[ \mathbb{E} \left[ F \left( 4 \sum_{(i, j) \in I \times I^c}^{} a_{ij}X_iX_j' \right) \right] \leq 
\mathbb{E} \left[ F \left( 4 \sum_{(i, j) \in [n] \times [n]}^{} a_{ij}X_iX_j' \right) \right] \]
where $[n] = \{1, \dots, n\}$. To do this, we can decompose the sum on the right side as 
\[ \sum_{(i, j) \in [n] \times [n]}^{} a_{ij}X_iX_j' 
= \underbrace{ \sum_{(i, j) \in I \times I^c}^{} a_{ij}X_iX_j' }_{Y} 
+ \underbrace{\sum_{(i, j) \in I \times I}^{} a_{ij}X_iX_j' 
+ \sum_{(i, j) \in I^c \times [n]}^{} a_{ij}X_iX_j'}_{Z}\]
Condition on all $(X_i)_{i \in I}$ and $(X_j')_{j \in I^c}$, and denote this expectation by $\mathbb{E}'$. 
This fixes $Y$, while $Z$ has zero conditional expectation (check). Thus, by Jensen inequality, we get 
\[ F(4Y) = F(4Y + \mathbb{E}'[4Z]) = F(\mathbb{E}'[4Y + 4Z]) \leq \mathbb{E}'[F(4Y + 4Z)]. \]
Finally, taking expectations over all remaining random variables, we get 
\[ \mathbb{E}[F(4Y)] \leq \mathbb{E}[F(4Y + 4Z)]. \]
Hence the proof is complete.
\end{proof}

\begin{remark}[Diagonal-free assumption]
\label{rmk:6.1.2}
The assumption is essential in \cref{thm:6.1.1}, since the conclusion fails for diagonal matrices when 
$F(x) = x$. But we can include the diagonal on the right hand side: for any $n \times n$ matrix $A = (a_{ij})$, 
we get 
\[ \mathbb{E}\left[ F \left( \sum_{i \neq j}^{} a_{ij}X_iX_j \right) \right] 
\leq \mathbb{E}\left[ F \left( 4 \sum_{i, j}^{} a_{ij}X_iX_j' \right) \right] \]
This is shown in Exercise 6.1, and there are other variants of decoupling (Exercises 6.2-6.4).
\end{remark}



% ----------6.2----------
\subsection{Hanson-Wright Inequality}
If $X$ is a subgaussian random vector in $\mathbb{R}^n$, what can we say about its norm? If $X$ has 
indepdendent entries, then it concentrated (\cref{thm:3.1.1}). But in general, it does not have to - it can be 
too small with high probability (Exercise 3.37). However, it can't be too large:

\begin{proposition}[Norm of subgaussian random vector]
\label{prop:6.2.1}
Let $X$ be a mean zero subgaussian random vector in $\mathbb{R}^n$ with $\lVert X \rVert_{\psi_2} \leq K$. Then 
for every $t \geq 0$, 
\[ P(\lVert X \rVert_{2} \geq CK(\sqrt{n} + t)) \leq e^{-t^2}. \]
\end{proposition}

\begin{proof}
WLOG, we can assume that $K = 1$. Squaring and exponentiating both sides and using Markov's inequality, we get 
\[ P(c \lVert X \rVert_{2} \geq \sqrt{n} + t) \leq e^{-(n + t^2)} \mathbb{E}\left[ \exp{(c^2 
\lVert X \rVert_{2}^2 )} \right]. \]
Now we will use a \textbf{Gaussian replacement} trick: for some absolute constant $c > 0$, we claim that 
\[ \mathbb{E}\left[ \exp{(c^2 \lVert X \rVert_{2}^2)} \right] \leq 
\mathbb{E}\left[ \exp{(\lVert g \rVert_{2}^2 / 6)} \right] \text{ where } g \sim N(0, I_n). \]
To see this, condition on $X$ (treating it as a fixed vector); then $\left\langle g, X \right\rangle \sim 
N(0, \lVert X \rVert_{2}^2)$ by \cref{cor:3.3.2}, so (using the MGF of a normal distribution)
\[ \mathbb{E}\left[ \exp{(c^2 \lVert X \rVert_{2}^2)} \right] = 
\mathbb{E}_g\left[ \exp{(\sqrt{2}c \left\langle g, X \right\rangle )} \right], \]
where $\mathbb{E}_g$ denotes the conditional expectation over $g$. Now takes expectation over $X$ over both 
sides and apply Fubini:
\[ \mathbb{E}_X\left[ \exp{(c^2 \lVert X \rVert_{2}^2)} \right] 
= \mathbb{E}_X\left[ \mathbb{E}_g\left[ \exp{(\sqrt{2}c \left\langle g, X \right\rangle )} \right] \right] 
= \mathbb{E}_g\left[ \mathbb{E}_X\left[ \exp{(\sqrt{2}c \left\langle X, g \right\rangle )} \right] \right]. \]
When we condition on $g$ (treating $g$ as a fixed vector), the subgaussian norm of $\left\langle X, g 
\right\rangle$ is at most 1 by assumption ($K = 1$), so \cref{prop:2.6.6} (iv) gives 
\[ \mathbb{E}_X\left[ \exp{(\sqrt{2}c \left\langle X, g \right\rangle )} \right] 
\leq \exp{(\lVert g \rVert_{2}^2 / 4)} \]
for some absolute constant $c > 0$. Substitute this into the bound above, then our claim for the Gaussian 
replacement is proved. After that, substitute into the first equation, and the proof is complete.
\end{proof}

The Gaussian replacement trick will also be useful when we are proving concentration regarding a chaos - namely, 
the Hanson-Wright inequality:
\begin{theorem}[Hanson-Wright inequality]
\label{thm:6.2.2}
Let $A$ be an $n \times n$ matrix. Let $X = (X_1, \dots, X_n) \in \mathbb{R}^n$ be a random vector with 
independent, mean-zero, subgaussian coordinates. Then, for every $t \geq 0$, we have
\[ P(|X^TAX - \mathbb{E}\left[ X^TAX \right]| \geq t) \leq 
2 \exp{\left[ -c \min_{}\left( \frac{t^2}{K^4 \lVert A \rVert_{F}^2}, \frac{t}{K^2 \lVert A \rVert_{}} 
\right) \right]}, \]
where $K = \max_{i} \lVert X_i \rVert_{\psi_2}$.
\end{theorem}

The proof will be based on bounding the MGF of $X^TAX$. Here is the plan:
\begin{enumerate}
	\item replace $X^TAX$ by $X^TAX'$ by decoupling;
	\item replace $X^TAX'$ by $g^T Ag'$ using Gaussian replacement, for $g \sim N(0, I_n)$;
	\item compute $g^TAg'$ by diagonalizing $A$ using the rotational invariance of $N(0, I_n)$.
\end{enumerate}

We start with part (b):
\begin{lemma}[Gaussian replacement]
\label{lem:6.2.3}
Let $A$ be an $n \times n$ matrix. Let $X$ be a mean zero, subgaussian random vector in $\mathbb{R}^n$ with 
$\lVert X \rVert_{\psi_2} \leq K$, and $X'$ be its independent copy. Let $g, g' \sim N(0, I_n)$ be independent. 
Then for any $\lambda \in \mathbb{R}$, 
\[ \mathbb{E}\left[ \exp{(\lambda X^TAX')} \right] \leq \mathbb{E}\left[ \exp{(CK^2 \lambda g^TAg')} \right]. \]
\end{lemma}

\begin{proof}
Condition on $X'$ and take expectation over $X$, which we denote $\mathbb{E}_X$. Then the random variable 
$\left\langle X, AX' \right\rangle$ is (conditionally) subgaussian, with subgaussian norm $\leq K 
\lVert AX' \rVert_{2}$. Then \cref{prop:2.6.6} (iv) gives 
\[ \mathbb{E}_X\left[ \exp{(\lambda X^TAX')} \right] \leq \exp{(C \lambda^2 K^2 \lVert AX' \rVert_{2}^2)}, 
\ \lambda \in \mathbb{R}. \]
Compare the above to the normal MGF formula. Applied to the normal random variable $g^TAX' = \left\langle 
g, AX' \right\rangle$ (still conditionally on $X'$), it gives 
\[ \mathbb{E}_g\left[ \exp{(\mu g^T AX')} \right] = \exp{(\mu^2 \lVert AX' \rVert_{2}^2 / 2)}, \ \mu 
\in \mathbb{R}. \]
Setting $\mu = \sqrt{2C}K \lambda$, we match the right hand sides of the two equations above and obtain 
\[ \mathbb{E}_X\left[ \exp{(\lambda X^TAX')} \right] = \mathbb{E}_g\left[ \exp{(\sqrt{2C}K \lambda g^TAX')} 
\right]. \]
Then, taking expectation over $X'$ on both sides, we see that we have replace $X$ by $g$ in the chaos, at a cost 
of the factor $\sqrt{2C}K$. Repeating the same thing for $X'$, we can replace $X'$ with $g'$ and get another 
factor of $\sqrt{2C}K$.
\end{proof}

We now move to step (c):
\begin{lemma}[MGF of a Gaussian quadratic form]
\label{lem:6.2.4} Let $A = (a_{ij})$ be an $n \times n$ matrix, and let $g, g' \sim N(0, I_n)$ be independent. 
Then 
\[ \mathbb{E}\left[ \exp{(\lambda g^TAg')} \right] \leq \exp{(\lambda^2 \lVert A \rVert_{F}^2)} 
\text{ whenever } |\lambda| \leq \frac{1}{2 \lVert A \rVert_{}}. \]
\end{lemma}

\begin{proof}
Let's use rotational invariance of the normal distribution do diagonalize $A$. With its singular value 
decomposition, $A = U \Sigma V^T$, we can write 
\[ g^T Ag' = (U^T g)^T \Sigma (V^T g'). \]
By the rotational invariance of the normal distribution (\cref{prop:3.3.1}), $U^T g$ and $V^T g'$ are 
independent standard normal random vectors in $\mathbb{R}^n$. So, 
\[ g^T Ag \sim g^T \Sigma g' = \sum_{i = 1}^{n} \sigma_i g_ig_i^T. \]
This is a sum of independent random variables, so 
\[ \mathbb{E}\left[ \exp{(\lambda g^TAg')} \right] 
= \mathbb{E}\left[ \prod_{i}^{} \mathbb{E}\left[ \exp{(\lambda \sigma_i g_ig_i^T)} \right] \right] 
= \prod_{i}^{} \mathbb{E}\left[ \exp{(\lambda \sigma_i g_ig_i^T)} \right]. \]
Now, for each $i$ and $t \in \mathbb{R}$, we have 
\[ \mathbb{E}\left[ \exp{(tg_ig_i')} \right] = \mathbb{E}\left[ \exp{\left( \frac{t^2 g_i^2}{2} \right)}
\right] = \frac{1}{\sqrt{1 - t^2}} \leq \exp{(t^2)} \text{ if } t^2 \leq \frac{1}{2}. \]
The first identity is done by conditioning on $g_i$ and using the MGF formula for the normal random variable 
$g'$; the other steps are just direct calculations. Substituting this bound with $t = \lambda \sigma_i$ into 
the product, we get 
\[ \mathbb{E}\left[ \exp{(\lambda g^T Ag')} \right] \leq \exp{\left( \lambda^2 \sum_{i}^{} \sigma_i^2 \right)} 
\text{ if } \lambda^2 \leq \frac{1}{2 \max_{} \sigma_i^2}. \]
Since $\sigma_i$ are the singular values of $A$, $\sum_{i}^{} \sigma_i^2 = \lVert A \rVert_{F}^2$ and 
$\max_{i} \sigma_i = \lVert A \rVert_{}$, hence the lemma is proved.
\end{proof}

Now we move to the main proof!
\begin{proof}[Proof of Hanson-Wright inequality]
Without loss of generality, assume $K = 1$. As usual, it is enough to bound the one-sided tail 
\[ p := P(X^TX - \mathbb{E}\left[ X^TAX \right] \geq t). \]
This is because we can find the lower tail by just replacing $A$ with $-A$. By combining the two tails, the 
proof would be complete.

In terms of the entries of $A = (a_{ij})$, we have 
\[ X^TAX = \sum_{i, j}^{}a_{ij}X_iX_j \text{ and } \mathbb{E}\left[ X^TAX \right] = \sum_{i}^{} a_{ii} 
\mathbb{E}\left[ X_i^2 \right], \]
where we used the mean zero assumption and independence. So 
\[ X^TAX - \mathbb{E}\left[ X^TAX \right] = \sum_{i}^{} a_{ii}(X_i^2 - \mathbb{E}\left[ X_i^2 \right]) 
+ \sum_{i \neq j}^{} a_{ij} X_iX_j. \]
The problem then reduces to estimating the diagonal and off-diagonal sums:
\[ p \leq P \left( \sum_{i}^{} a_{ii}(X_i^2 - \mathbb{E}\left[ X_i^2 \right]) \geq t/2 \right) 
+ P \left( \sum_{i \neq j}^{} a_{ij} X_iX_j \geq t/2 \right) := p_1 + p_2. \]

Let's bound these probabilities!

\textbf{Step 1: Diagonal sum.} Since $X_i$ are independent and subgaussian, $X_i^2 - \mathbb{E}\left[ X_i^2 
\right]$ are independent, mean-zero, and subexponential. Also, 
\[ \lVert X_i^2 - \mathbb{E}\left[ X_i^2 \right] \rVert_{\psi_2} \lesssim \lVert X_i^2 \rVert_{\psi_1} 
\lesssim \lVert X_i \rVert_{\psi_2}^2 \lesssim 1. \]
(The above follows from centering andcref{lem:2.8.5}). Then, Bernstein's inequality (\cref{cor:2.9.2}) gives 
\[ p_1 \leq \exp{\left[ -c \min_{}\left( \frac{t^2}{\sum_{i}^{}a_{ii}^2}, \frac{t}{\max_{i}|a_{ii}|} \right) 
\right]} \leq \exp{\left[ -c \min_{}\left( \frac{t^2}{\lVert A \rVert_{F}^2}, \frac{t}{\lVert A \rVert_{}} 
\right) \right]}. \]

\textbf{Step 2: Off-diagonal sum.} Now we bound the off-diagonal sum
\[ S := \sum_{i \neq j}^{}a_{ij}X_iX_j. \]
Let $\lambda > 0$ be a parameter to be determined later. By Merkov's inequality, we have 
\[ p_2 = P(S \geq t/2) = p(\lambda S \geq \lambda t/2) \leq \exp{(-\lambda t/2)}\mathbb{E}\left[ 
\exp{(\lambda S)} \right]. \]
We get 
\begin{align*}
	\mathbb{E}\left[ \exp{(\lambda S)} \right] 
	&\leq \mathbb{E}\left[ \exp{(4 \lambda X^TAX')} \right] \quad (\text{By decoupling}) \\
	&\leq \mathbb{E}\left[ \exp{(C_1 \lambda g^TAg')} \right] \quad (\text{By \cref{lem:6.2.3}}) \\
	&\leq \exp{(C \lambda^2 \lVert A \rVert_{F}^2)} \quad (\text{By \cref{lem:6.2.4}})
\end{align*}
whenever $|\lambda| \leq \frac{1}{2 \lVert A \rVert_{}}$. Putting this bound into the exponential bound above, 
we obtain 
\[ p_2 \leq \exp{(-\lambda t/2 + C \lambda^2 \lVert A \rVert_{F}^2)}. \]
Optimizing over $0 \leq \lambda \leq \frac{1}{2 \lVert A \rVert_{}}$, we conclude that 
\[ p_2 \leq \exp{\left[ -c \min_{}\left( \frac{t^2}{\lVert A \rVert_{F}^2}, \frac{t}{\lVert A \rVert_{}}
\right) \right]}. \]
To summarize, we obtained the desired bounds for the probabilities of the diagonal deviation $p_1$ and the 
off-diagonal deviation $p_2$. Putting them together, we complete the proof of \cref{thm:6.2.2}.
\end{proof}



% ----------6.3----------
\subsection{Symmetrization}
A random variable $X$ is called \underline{symmetric} if it has the same distribution as $-X$. A basic example 
is the Rademacher random variable, which takes values $-1$ and $1$ with equal probabilities. Mean-zero normal 
random variables are also symmetric, while the exponential and Poisson distributions are not.

This section introduces \underline{symmetrization}, a useful trick for reducing problems to symmetric 
distributions - and sometimes even to the Rademacher distribution. It is based on the following:

\begin{lemma}[Constructing symmetric distributions]
\label{lem:6.3.1}
Let $X$ be a random variable and $\xi$ be an independent Rademacher random variables. Then 
\begin{enumerate}
	\item $\xi X$ and $\xi |X|$ are identically distributed and symmetric.
	\item If $X$ is symmetric, both $\xi X$ and $\xi |X|$ have the same distribution as $X$. 
	\item If $X'$ is an independent copy of $X$, then $X - X'$ is symmetric.
\end{enumerate}
\end{lemma}

\begin{proof}
We'll check that $\xi X$ is symmetric. For any interval $A \subset \mathbb{R}$, the law of total probability 
gives 
\begin{align*}
	P(\xi X \in A) 
	&= P(\xi X \in A | \ \xi = 1) \cdot \frac{1}{2} + P(\xi X \in A | \ \xi = -1) \cdot \frac{1}{2} \\
	&= \frac{1}{2}(P(X \in A) + P(-X \in A)).
\end{align*}
Let's also do this for $-\xi X$: 
\begin{align*}
	P(-\xi X \in A) 
	&= P(-\xi X \in A | \ \xi = 1) \cdot \frac{1}{2} + P(-\xi X \in A | \ \xi = -1) \cdot \frac{1}{2} \\
	&= \frac{1}{2}(P(-X \in A) + P(X \in a)).
\end{align*}
Therefore $\xi X$ and $-\xi X$ have the same CDF, meaning they have the same distribution.

The rest of the proof is in Exercise 6.16.
\end{proof}

\begin{lemma}[Symmetrization]
\label{lem:6.3.2}
Let $X_1, \dots, X_N$ be independent, mean zero random vectors in a normed space, and let $\varepsilon_1, 
\dots, \varepsilon_N$ be independent Rademacher random variables. Then 
\[ \frac{1}{2}\mathbb{E}\left[ \left\lVert \sum_{i = 1}^{N} \varepsilon_i X_i \right\rVert_{} \right] 
\leq \mathbb{E}\left[ \left\lVert \sum_{i = 1}^{N} X_i \right\rVert_{} \right] 
\leq 2 \mathbb{E}\left[ \left\lVert \sum_{i = 1}^{N} \varepsilon_i X_i \right\rVert_{} \right]. \]
\end{lemma}

\begin{proof}
(\textbf{Upper bound}) Let $(X_i')$ be an independent copy of $(X_i)$. Since $\sum_{i = 1}^{} X_i'$ has mean 
zero, we have 
\[ p := \mathbb{E}\left[ \left\lVert \sum_{i}^{} X_i \right\rVert \right] 
\leq \mathbb{E}\left[ \left\lVert \sum_{i}^{} X_i - \sum_{i}^{} X_i' \right\rVert \right] 
= \mathbb{E}\left[ \left\lVert \sum_{i}^{} (X_i - X_i') \right\rVert \right]. \]
The inequality above comes from the fact that for independent random vectors $Y$ and $Z$, 
\[ \mathbb{E}[Z] = 0 \implies \mathbb{E}[\lVert Y \rVert_{}] \leq \mathbb{E}[\lVert Y + Z \rVert_{}]. \]
Since $X_i - X_i'$ are symmetric random vectors, they have the same distribution as 
$\varepsilon_i (X_i - X_i')$ by \cref{lem:6.3.1} (b). Then 
\begin{align*}
	p 
	&\leq \mathbb{E}\left[ \left\lVert \sum_{i}^{} \varepsilon_i (X_i - X_i') \right\rVert \right] \\
	&\leq \mathbb{E}\left[ \left\lVert \sum_{i}^{} \varepsilon_i X_i \right\rVert \right] 
	+ \mathbb{E}\left[ \left\lVert \sum_{i}^{} \varepsilon_i X_i' \right\rVert \right] \quad 
	\text{(Triangle inequality)} \\
	&= 2 \mathbb{E}\left[ \left\lVert \sum_{i}^{} \varepsilon_i X_i \right\rVert \right] \quad 
	\text{(The two terms are identically distributed)}.
\end{align*}

(\textbf{Lower bound}) The argument is similar as the proof for the upper bound:
\begin{align*}
	\mathbb{E}\left[ \left\lVert \sum_{i}^{} \varepsilon_i X_i \right\rVert \right] 
	&\leq \mathbb{E}\left[ \left\lVert \sum_{i}^{} \varepsilon_i (X_i - X_i') \right\rVert \right] \\
	&= \mathbb{E}\left[ \left\lVert \sum_{i}^{} (X_i - X_i') \right\rVert \right] \quad 
	\text{(Same distribution)} \\
	&\leq \mathbb{E}\left[ \left\lVert \sum_{i}^{} X_i \right\rVert \right] 
	+ \mathbb{E}\left[ \left\lVert \sum_{i}^{} X_i' \right\rVert \right] \quad \text{(Triangle inequality)} \\
	&= 2 \mathbb{E}\left[ \left\lVert \sum_{i}^{} X_i \right\rVert \right] \quad 
	\text{(Identical distribution)}.
\end{align*}
Question: Where did we use $X_i$'s independence? Do we need mean zero for both upper and lower bounds?
\end{proof}

There are also other versions of symmetrization lemmas (Exercises 6.19-6.21).



% ----------6.4----------
\subsection{Random Matrices with non-i.i.d. Entries}
A typical application of symmetrization consist of two steps: First, replace random variables $X_i$ with 
symmetric ones $\varepsilon_i X_i$, then condition on $X_i$ so that all randomness comes from the signs 
$\varepsilon_i$. Hence this reduces the problems to Rademacher random variables. To illustrate this techinique, 
let's bound the operator norm of a random matric with independent, non-identically distributed entries:

\begin{theorem}[Norm of random matrices with non-i.i.d. entries]
\label{thm:6.4.1}
Let $A$ be an $n \times n$ symmetric random matrix with independent, mean zero entries above and on the 
diagonal. Then 
\[ \mathbb{E}\left[ \max_{i} \lVert A_i \rVert_{2} \right] \leq \mathbb{E}\left[ \lVert A \rVert_{} \right] 
\leq C \sqrt{\log_{}{n}} \cdot \mathbb{E}\left[ \max_{i} \lVert A_i \rVert_{2} \right], \]
where $A_i$ denotes the rows of $A_i$.
\end{theorem}

\begin{proof}
The lower bound is already done in Exercise 4.7.

For the upper bound, we will use symmetrization and the matrix Khintchine inequality (\cref{thm:5.4.14}). 

Let's decompose $A$ entry-by-entry, keeping symmetry in mind, like the proof of \cref{thm:5.5.1}. Denote the 
standard basis of $\mathbb{R}^n$ by $e_1, \dots, e_n$, then $A$ can be expressed as a sum of independent, 
mean zero random matrices:
\[ A = \sum_{i \leq j}^{} Z_{ij}, \text{ where } Z_{ij} = \begin{cases}
	A_{ij}(e_i e_j^T + e_j e_i^T) &\text{ if } i < j, \\
	A_{ii}e_i e_i^T &\text{ if } i = j
\end{cases}. \]
By applying symmetrization (\cref{lem:6.3.2}), we get 
\[ \mathbb{E}\left[ \lVert A \rVert_{} \right] 
= \mathbb{E}\left[ \left\lVert \sum_{i \leq j}^{} Z_{ij} \right\rVert \right] 
\leq 2 \mathbb{E}\left[ \left\lVert \sum_{i \leq j}^{} \varepsilon_{ij} Z_{ij} \right\rVert \right] 
\quad (*) \]
where $\varepsilon_{ij}$ are independent Rademacher random variables.

Condition on $(Z_{ij})$, apply the matrix Khintchine inequality (\cref{thm:5.4.14}) for $p = 1$, and take 
expectation over $(Z_{ij})$ using the law of total expectation, which gives 
\[ \mathbb{E}\left[ \left\lVert \sum_{i \leq j}^{} \varepsilon_{ij} Z_{ij} \right\rVert \right] 
\leq C \sqrt{\log_{}{n}}\mathbb{E}\left[ \left\lVert \sum_{i \leq j}^{}Z_{ij}^2 \right\rVert^{1/2} \right]. 
\quad (**) \]
Since $(Z_{ij})$ is a diagonal matrix, 
\[ Z_{ij}^2 = \begin{cases}
	A_{ij}^2 (e_i e_j^T + e_je_i^T) &\text{ if } i < j, \\
	A_{ii}^2 e_i e_i^T &\text{ if } i = j
\end{cases}. \]
Therefore, 
\[ \sum_{i \leq j}^{} Z_{ij}^2 
= \sum_{i = 1}^{n} \left( \sum_{j = 1}^{n} A_{ij}^2 \right) e_ie_i^T 
= \sum_{i = 1}^{n} \lVert A_i \rVert_{2}^2 e_i e_i^T. \]
In other words, this is a diagonal matrix with diagonal entries equal to $\lVert A_i \rVert_{2}^2$. Since 
the operator norm of a diagonal matrix is the maximal absolute value of its entries, we get 
\[ \left\lVert \sum_{i \leq j}^{} Z_{ij}^2 \right\rVert = \max_{i} \lVert A_i \rVert_{2}^2. \]
Substitute the bound above into (**) then into (*) completes the proof.
\end{proof}

There is more practice on symmetrization as well (Exercises 6.22-6.29).



% ----------6.5----------
\subsection{Application: Matrix Completion}




% ----------6.6----------
\subsection{Contraction Principle}
There is one more useful inequality the text covers in the chapter:

\begin{theorem}[Contraction principle]
\label{thm:6.6.1}
Let $x_1, \dots, x_N$ be any vectors in a normed space, $(a_1, \dots, a_N) \in \mathbb{R}^N$, and 
$\varepsilon_1, \dots, \varepsilon_N$ be independent Rademacher random variables. Then 
\[ \mathbb{E}\left[ \left\lVert \sum_{i = 1}^{N} a_i \varepsilon_i x_i \right\rVert \right] 
\leq \lVert a \rVert_{\infty} \cdot \mathbb{E}\left[ \left\lVert 
\sum_{i = 1}^{N} \varepsilon_i x_i \right\rVert \right]. \]
\end{theorem}

\begin{proof}
WLOG, assume that $\lVert a \rVert_{\infty} \leq 1$. Define the function 
\[ f(a) := \mathbb{E}\left[ \left\lVert \sum_{i = 1}^{N} a_i \varepsilon_i x_i \right\rVert \right]. \]
Then $f: \mathbb{R}^n \to \mathbb{R}$ is convex (Exercise 6.35).

We want to bound for $f$ the set of points $a$ satisfying $\lVert a \rVert_{\infty} \leq 1$, i.e. on the unit 
cube $[-1, 1]^N$. By the maximum principle (Exercises 1.4 \& 1.5), the maximum of a convex function on the 
cube is attained at a vertex, where all $a_i = \pm 1$. For such $a$, the random variables $(\varepsilon_i a_i)$ 
have the same distribution as $\varepsilon_i$ by symmetry. Thus 
\[ \mathbb{E}\left[ \left\lVert \sum_{i = 1}^{N} a_i \varepsilon_i x_i \right\rVert \right] 
= \mathbb{E}\left[ \left\lVert \sum_{i = 1}^{N} \varepsilon_i x_i \right\rVert \right], \]
thus 
\[ f(a) \leq \mathbb{E}\left[ \left\lVert \sum_{i = 1}^{N} \varepsilon_i x_i \right\rVert \right] 
\text{ whenever } \lVert a \rVert_{\infty} \leq 1, \]
which completes the proof.
\end{proof}

As an application, we can prove a version of symmetrization but with Gaussian random variables 
$g_i \sim N(0, 1)$ instead of Rademachers.

\begin{lemma}[Symmetrization with Gaussians]
\label{lem:6.6.2}
Let $X_1, \dots, X_N$ be independent, mean zero random vectors in a normed space. Let $g_1, \dots, g_N 
\sim N(0, 1)$ be independent Gaussian random variables, which are also independent of $X_i$. Then 
\[ \frac{c}{\sqrt{\log_{}{N}}} \mathbb{E}\left[ \left\lVert \sum_{i = 1}^{N} g_iX_i \right\rVert \right] 
\leq \mathbb{E}\left[ \left\lVert \sum_{i = 1}^{N} X_i \right\rVert \right] 
\leq 3 \mathbb{E}\left[ \left\lVert \sum_{i = 1}^{N} g_i X_i \right\rVert \right]. \]
\end{lemma}

\begin{proof}
\textbf{(Upper bound)} By symmetrization (\cref{lem:6.3.2}), we have 
\[ E := \mathbb{E}\left[ \left\lVert \sum_{i = 1}^{N} X_i \right\rVert \right] 
\leq 2 \mathbb{E}\left[ \left\lVert \sum_{i = 1}^{N} \varepsilon_i X_i \right\rVert \right]. \]
To interject Gaussian random variables, recall that $\mathbb{E}\left[ |g_i| \right] = \sqrt{2 / \pi}$. Then 
we can continue the bound as follows: 
\begin{align*}
	E 
	&\leq 2 \sqrt{\frac{\pi}{2}} \mathbb{E}_X\left[ \left\lVert \sum_{i = 1}^{N} \varepsilon_i 
	\mathbb{E}_g\left[ |g_i| \right] X_i \right\rVert \right] \\
	&\leq 2 \sqrt{\frac{\pi}{2}} \mathbb{E}\left[ \left\lVert \sum_{i = 1}^{N} 
	\varepsilon_i |g_i| X_i \right\rVert \right] \quad \text{(Jensen inequality)} \\
	&= 2 \sqrt{\frac{\pi}{2}} \mathbb{E}\left[ \left\lVert \sum_{i = 1}^{N} 
	g_i X_i \right\rVert \right]. 
\end{align*}
The last equality holds since the random variables $(\varepsilon_i |g_i|)$ have the same joint distribution as 
$(g_i)$ (\cref{lem:6.3.1} (b)).

\textbf{(Lower bound)} We have 
\begin{align*}
	\mathbb{E}\left[ \left\lVert \sum_{i = 1}^{N} g_i X_i \right\rVert \right] 
	&= \mathbb{E}\left[ \left\lVert \sum_{i = 1}^{N} \varepsilon_i g_i X_i \right\rVert \right] \quad 
	\text{(Symmetry of $g_i$)} \\
	&\leq \mathbb{E}_g\left[ \mathbb{E}_X\left[ \lVert g \rVert_{\infty} 
	\mathbb{E}_{\varepsilon}\left[ \left\lVert \sum_{i = 1}^{N} \varepsilon_i X_i 
	\right\rVert \right] \right] \right] \quad \text{(\cref{thm:6.6.1})} \\
	&= \mathbb{E}_g\left[ \lVert g \rVert_{\infty} \mathbb{E}_{\varepsilon}\left[ \mathbb{E}_X\left[ 
	\left\lVert \sum_{i = 1}^{N} \varepsilon_i X_i \right\rVert \right] \right] \right] \quad 
	\text{(Independence)} \\
	&\leq 2 \mathbb{E}_g\left[ \lVert g \rVert_{\infty} \mathbb{E}_X\left[ \left\lVert 
    \sum_{i = 1}^{N} X_i\right\rVert \right] \right] \quad \text{(\cref{lem:6.3.2})} \\
	&= 2 \mathbb{E}\left[ \lVert g \rVert_{\infty} \right] \cdot 
	\mathbb{E}\left[ \left\lVert \sum_{i = 1}^{N} X_i \right\rVert \right] \quad \text{(Independence).}
\end{align*}
Moreover, by \cref{prop:2.7.6}, 
\[ \mathbb{E}\left[ \lVert g \rVert_{\infty} \right] \leq C \sqrt{\log_{}{N}}. \]
Plugging back gives the result.
\end{proof}

\begin{remark}[Log factor is unavoidable]
\label{rmk:6.6.3}
The logarithmic factor in \cref{lem:6.6.2} is necessary and optimal in general (Exercise 6.37), making 
Gaussian symmetrization weaker than Rademacher's.
\end{remark}
